============================================================================
Google Business Intelligence Professional Certificate
============================================================================

A large part of a BI professional's work revolves around identifying how current systems and processes operate, evaluating potential improvements, and implementing them so that the current system is closer to the ideal system state.



============================================================================
1. Foundations of Business Intelligence

Discover the role of BI professionals within an organization and the career paths they typically follow. Then, explore core BI practices and tools and learn how BI professionals use them to make a positive impact on organizations. 


----------------------------------------------------------------------------
Module 1: Data-driven results through business intelligence

Then, you will begin learning about the BI industry and the roles of BI analysts and engineers. You will be introduced to BI tools and techniques for making business decisions and improving processes. Finally, you will learn the similarities and differences between the two fields of BI and data analytics. 


Business intelligence drives change:


a business’s ability to identify issues before they become problems or act on opportunities before their competition is key to intelligent decision-making. Now more than ever, we have access to data about our marketplace, organizations, customers, competitors, and employees. But in order to turn that data into rapid results, we need business intelligence. Business intelligence involves automating processes and information channels in order to transform relevant data into actionable insights that are easily available to decision-makers. 

Restaurants reducing waste :


Consider a fictitious scenario about a fast-food restaurant chain. Leaders at this company have huge amounts of data to manage, such as:

customer transactions

marketing data related to promotions

customer satisfaction

employee information

And so much more! But on top of all of that, the company also has to consider the logistics for individual restaurants. That’s where the problem comes in.



The problem
The restaurants need to have ingredients to cook and serve customers, but if they have too much, that extra stock often goes to waste. Company leadership consults with their BI team to consider how to approach two concerns:

How to ensure the restaurants’ numerous locations have enough ingredients to meet customer demand

How to reduce food waste

However, these stakeholders currently don’t have metrics in place to specifically measure food waste or strategies to reduce it. This is exactly where the BI team will need to start.




The solution
In order to address the stakeholder’s needs, the BI team spends time gathering information about current metrics and processes. They first use this information to determine what data they have and how it’s being used. They discover that there are already useful metrics being applied in other ways by various teams in the company, including:

How many ingredients are delivered to each location

How much of each menu item is made each day

How much of each menu item is actually being ordered each day


The results
Knowing how much food is actually going to waste now enables stakeholders to better achieve their goals. The restaurant chain discovers that the largest source of food waste is the French fries. Across their locations, 10–20% of French fries are left over at the end of the month. With this information, the company’s central operations team sends out a memo to all branches recommending they reduce their incoming French fry delivery by 10%. In this way, the BI analysts are able to help the business identify an area for improvement and reduce waste. 


No matter what industry you’re working in, BI can automate processes and information channels to empower the people who need that data to answer questions and make decisions. From restaurants reducing waste to hospitals advancing patient care, BI analysts create systems and tools to anticipate needs and enable organizations to reach their objectives. 

Data governance professionals are responsible for the formal management of an organization’s data assets. This may involve managing the availability, integrity, and security of data based on internal standards and policies.




Collaboration with business intelligence partners:

Previously, you learned about the many different partners a business intelligence professional might team up with to create systems and tools for an organization to improve processes and provide stakeholders with ongoing insights. These partners could include:

API professionals

Data warehousing specialists

Data governance professionals

Data analysts

IT professionals

Project managers

And many more!




Managing membership data
The marketing team for a retail store was looking for ways to increase memberships for its loyalty program. Leaders wanted to encourage repeat customers to enroll. However, they didn’t have a system in place that allows analysts to explore both member and nonmember sales data. This requires ingesting data from a variety of systems, including the store’s online membership form and sales data. So, in addition to building a tool that moves and transforms key data, the BI team also needed to make the data from different systems align with the destination system. 

In this case, they collaborated with several teams:

The marketing team: The marketing team was the primary stakeholder for this initiative. They worked with the BI team to determine project requirements, timelines, and deliverables.

The API team: Next, the BI team collaborated with the API professionals in order to integrate the data into the internal company database. The API team also helped build the reporting tools and dashboards. 

Data warehousing specialists: Then they teamed up with data warehousing specialists to create a storage and organization system for the newly acquired data.


Collaborating with the people on your team who have different skills and perspectives is an important part of a structured approach to BI. As a BI professional, you will collaborate with a variety of partners to create systems that empower stakeholders with data to advance and succeed. 



Business stakeholder initiates project
The chief operating officer (COO) sends you an email asking for a way to track how the company’s travel spending compares to the travel budget for the current fiscal year.


Project manager defines the work
The project manager starts gathering requirements, defines the project scope, and prepares the timeline. You have daily check-ins with them to confirm alignment.


You find the right data
As the business intelligence professional on the project, you meet with the sales and finance team. You learn that company travel details are in an in-house expense system, and budgets are in the company’s planning software.


Engineer helps access the data
Now that you know where the data is, you need access. You ask an application programming interface engineer to help get the data. They write Python programs that pull data from the two systems and deliver it into a data warehouse.


Data specialists clean the data
Now that the raw spend and budget data is in a warehouse, data warehousing specialists clean and standardize the data, validate it, and deliver it into a BigQuery database.


Review for data governance
With the data available as analyst-ready tables in a BigQuery database, a data governance professional makes sure the data has proper access controls in place.


Analyze the data with data analysts
Working closely with data analysts, you understand the data, identify key fields, and conduct a preliminary analysis. Then, you merge travel history data with budget data in order to answer your COO’s question about how travel spend compares with the budget.


Create a dashboard and present results
Because the COO also wants to monitor travel spend going forward, you build an insights dashboard that’s connected to the data warehouse. Then, you prepare a short slide presentation to share the results of the project with the COO.


Business intelligence analysts gather requirements from stakeholders, partners, and team members. They use their understanding of large datasets to retrieve, organize, and interpret data. Then, they create visualizations, dashboards, and reports. Business intelligence engineers evaluate and streamline devices, infrastructures, and information channels.




To enable different computer programs to communicate with one another, companies can use an application programming interface, or API. This is a set of functions and procedures that integrate diverse systems. 



Business intelligence professionals collaborate with information technology, or IT, professionals in order to maximize available data and data tools. IT professionals test, install, repair, upgrade, and maintain the hardware and software solutions.


When helping their organizations achieve data maturity, business intelligence professionals are typically responsible for which of the following tasks?

Business intelligence professionals are typically more involved with building data-reporting tools and dashboards, establishing repeatable methods for monitoring data, and  working on large-scale projects that are helpful to multiple stakeholders.


BI professionals evaluate the data needs of their stakeholders, identify necessary sources, and design pipeline systems that automatically and continuously gather that data for stakeholders to access.



Gathering Data;
BI professionals build tools that clean and transform data automatically within a pipeline so that these processes occur to all data being ingested by the pipeline process.


Storage system;
BI professionals develop storage systems that allow intake from multiple source systems into a destination database, while governing the database schema and optimizing the system.


Descriptive and predictive analytics:
BI uses analysis of historical trends to perform predictive analytics that enable organizations to determine likely future trends and act accordingly.


Presenting insights;
BI analysts create tables, reports, and dashboards that empower stakeholders with access to the data they need to inform their whole decision-making process.




three stages of Business Intelligence (BI): Capture, Analyze, and Monitor, emphasizing how each stage contributes to gaining valuable insights from data.

Understanding the Stages

Capture: This initial stage focuses on gathering historical data, like past customer purchases or financial records. While essential, this stage alone doesn't offer in-depth insights for future planning.
Analyze: This stage delves deeper into the "why" behind the data, uncovering relationships and patterns to understand the reasons behind past trends and outcomes.

Leveraging Insights
Monitor: This stage utilizes BI tools like dashboards and data models to transform real-time data into actionable intelligence. This empowers stakeholders to make proactive, data-driven decisions aligned with business goals.


The business intelligence phases of capture, analyze, and monitor help determine BI’s business value, as well as an organization’s data maturity level. Data maturity is the extent to which an organization is able to use its data in order to extract actionable insights. 



In business intelligence, the capture stage might involve querying a database to return a financial dataset or accessing a spreadsheet of marketing campaign data. This stage pertains to static, backward-looking data. 


Data analysis may be used to help business intelligence professionals provide data-driven insights in many ways. Some examples include exploring why things happened, understanding relationships between data points, and examining data more in-depth.


how important a well-defined business intelligence (BI) strategy is for organizations to successfully use data for their goals.

Effective BI Strategy

Just like in sports or games like chess, BI benefits greatly from a clear strategy to guide its use.
A successful BI strategy involves managing people, processes, and tools in a coordinated way.
Key Elements of BI Strategy

People: Communication and collaboration across teams are essential. It's important to define roles, understand the vision for BI, and how it aligns with overall business goals.
Process: This involves determining which BI solutions are being used, how they're being used, and what future solutions might be needed. User support and training are also crucial aspects of this element.
Tools and KPIs

Tools: When choosing BI tools, always prioritize the needs of the users. Consider which dashboards, reports, and technologies will be most effective for different teams and departments.
KPIs: Key performance indicators (KPIs) should be established to measure the success of BI initiatives. These KPIs should be quantifiable and directly linked to business objectives.
Remember, a well-defined BI strategy ensures everyone is working together effectively and using data-driven insights to achieve shared goals.



Key business intelligence documents:
Previously, you learned about business intelligence strategy, which is the management of the people, processes, and tools used in the business intelligence process. BI projects are complicated, and finding ways to stay organized from the beginning of a project to the end is key to success. One way to ensure that you capture the big-picture project requirements, stay organized, and make an impact at your organization is to create comprehensive BI documents. In this reading, you’ll learn about three types of documents: the Stakeholder Requirements Document, Project Requirements Document, and Strategy Document.

1. Stakeholder Requirements Document
The Stakeholder Requirements Document enables you to capture stakeholder requests and requirements so you understand their needs before planning the rest of the project details or strategy. It should answer the following questions:

Business problem: What is the primary question to be answered or problem to be solved?

Stakeholders: Who are the major stakeholders of this project, and what are their job titles?

Stakeholder usage details: How will the stakeholders use the BI tool?

Primary requirements: What requirements must be met by this BI tool in order for this project to be successful?



Here are some questions BI professionals ask in order to successfully complete this document:

What questions must be answered before starting this project?

What does the BI team need to know before starting this project?

What are the questions that must be answered/problems that must be solved by this project?

What datasets are considered important to this project?

Who should have access to the dashboard? Will the entire dashboard be visible to all stakeholders?




Project Requirements Document
Once you have established the stakeholder requirements, you can start thinking about the project requirements that need to be met to achieve the stakeholder requirements. The Project Requirements Document contains the following details:

Purpose: Briefly describe why this project is happening and explanation of why the company should invest its resources in it.

Key dependencies: Detail the major elements of this project. Include the team, primary contacts, and expected deliverables. Are there any inter-team deliverables required?

Stakeholder requirements: List the established stakeholder requirements, based on the Stakeholder Requirements Document. Prioritize the requirements as: R - required, D - desired, or N - nice to have.

Success criteria: Clarify what success looks like for this project. Include explicit statements about how to measure success. Use SMART criteria.

User journeys: Document the current user experience and the ideal future experience.

Assumptions: Explicitly and clearly state any assumptions you are making.

Compliance and privacy: Include compliance, privacy, or legal dimensions to consider.

Accessibility: List key considerations for creating accessible reports for all users. Who needs to access this feature? How are they viewing and interacting with it?

Roll-out plan: Briefly describe the expected scope, priorities and timeline. Consider at what points during the rollout will measurements be made to determine whether the feature is performing as expected? Is there a rollback plan and timeline if this feature does not meet its intended goals?

In addition, some companies will ask you to include a list of references. If so, it’s a best practice to be liberal in citing references; you can never have too many. References might include:

Documents or websites you read and researched while working on this project

Laws and policies: Any regulations driving the project requirements

Project tracking: A link to tracking spreadsheet, bug hotlist, etc.

Similar projects: A description of anything similar that has been attempted in the past or any parallel efforts.



Strategy Document
Finally, you will create a Strategy Document for your project. This is the final phase of the planning process. The Strategy Document is a collaborative place to align with stakeholders about project deliverables. You will work together to establish information about dashboard functionality and associated metrics and charts.

This is a time to flesh out what metrics will be required, how metrics are calculated, and any limitations or assumptions that exist about the data. Stakeholders think through these details and help the BI professional make final project decisions. Then, the BI professional provides stakeholders with a dashboard mockup to get valuable feedback.

Generally, the BI professional will create the document and request review and sign-off from important stakeholders. Then they can begin working on the project with all of the details they need


Staying organized and aligned with stakeholders is an important part of the BI process. Creating documents early on in a project to outline stakeholder and project requirements as well as project strategies can be an important tool for a BI professional aligning with stakeholders and planning ahead. Soon, you’ll have an opportunity to create your own documents to align with stakeholders and plan your end-of-course project! 




Activity: Complete the business intelligence documents:


Meeting Notes: 
Stakeholders: 

Alice Shi, Vice President of Sales

Matías Sosa, Program Manager

Team members: 

Ariana Tirado, Data Warehousing Specialist

Cornelia Vega, Manager, Data Governance 

Sam Winters, Data Analyst

Dashboard needs to be accessible, with large print and text-to-speech alternatives

Background info:

MarkIt maintains an online platform that facilitates previously-owned item sales between individual buyers and sellers. The goal is to understand how these buyers and sellers use their platform. The insights could then inform new-product design and improve the platform. 

The team wants to review data that tracks the number of listings posted; the number of sales completed; and the number of listings deleted on a daily, quarterly, and yearly timescale. They also want to better understand search query behavior that buyers have when searching for an item. For example, if a buyer searches for more different types of items, are they more or less likely to complete a purchase? Do broader search terms mean a user is less committed to making a purchase?

Project goals:

Understand what customers want, what makes a successful sale, and how to improve experience for buyers and sellers

Understand how the platform is used by both types of users: How much time do users spend on the site? What pages do they spend the most time on? How do buyers conduct searches, and how do sellers create and maintain listings? How do buyers and sellers contact one another?

Discover how we can apply insights related to search query behavior

Understand pain points in the sales process

The ask/metrics:

Include fields for customer ID/username, item category (such as clothing or household goods), and date.

Determine if we can add a chart illustrating how long the listings for completed sales are online before the sale is completed.

For buyers: Include a chart comparing the number of searches made and the number of sales completed.


The Stakeholder Requirements Document enables you to capture stakeholder requests and requirements so you understand their needs before planning the rest of the project details or strategy. It should answer the following questions:


Business problem: What is the primary question to be answered or problem to be solved?

Stakeholders: Who are the major stakeholders of this project, and what are their job titles? 

Stakeholder usage details: How will the stakeholders use the BI tool?

Primary requirements: What requirements must be met by this BI tool in order for this project to be successful? 

Here are some questions BI professionals ask to successfully complete this document:

What questions must be answered before starting this project? 

What does the BI team need to know before starting this project? 

What are the questions that must be answered and/or problems that must be solved by this project?

What datasets are considered important to this project?

Who should have access to the dashboard? Will the entire dashboard be visible to all stakeholders?

Typically, the Stakeholder Requirements Document is a one-page document with notes, but it can be longer and more detailed for complex projects. 


The Project Requirements Document contains the following details:

Purpose: Briefly describe why this project is happening and explain why the company should invest its resources in it.

Key dependencies: Detail the major elements of this project. Include the team, primary contacts, and expected deliverables. Are any inter-team deliverables required? 

Stakeholder requirements: List the established stakeholder requirements, based on the Stakeholder Requirements Document. Prioritize the requirements as: R (required), D (desired), or N (nice to have).

Success criteria: Clarify what success looks like for this project. Include explicit statements about how to measure success. Use SMART criteria. You can review 
SMART criteria in the Google Data Analytics Certificate.
 


User journeys: Document the current user experience and the ideal future experience. 

Assumptions: Explicitly and clearly state any assumptions you are making. 

Compliance and privacy: Include any compliance, privacy, or legal dimensions to consider. 

Accessibility: List key considerations for creating accessible reports for all users. Who needs to access this feature? How are they viewing and interacting with it? 


What to Include in Your Response
Be sure to address the following elements in your completed planning documents: 

A Stakeholder Requirements Document, completed as fully as possible with information from your notes 

A Project Requirements Document, completed as fully as possible with information from your notes 

A Strategy Document, completed as fully as possible with information from your notes 

At least three questions about missing information from the interview notes



The Stakeholder Requirements Document enables you to capture stakeholder requirements so you understand their needs before planning the rest of the project details or strategy.



The Project Requirements Document should briefly describe the purpose of the project and explain why the company should invest its resources in it. It should also contain details about the project’s key dependencies, stakeholder requirements, success criteria, user journeys, assumptions, compliance and privacy concerns, accessibility issues, and roll-out plan.

The Strategy Document is a collaborative place to align with stakeholders about required metrics and charts, dashboard functionality, and project deliverables.




Activity Exemplar: Complete the business intelligence documents:


Assessment of Exemplar

In this activity, you practiced completing key BI planning documents based on a realistic scenario. The scenario was intentionally missing important information, so part of your task was to identify what you were missing and come up with follow-up questions you could ask the stakeholder. This activity will prepare you to complete these forms in future projects throughout your career.

Project planning docs help you understand the needs and expectations of your stakeholders. This allows you to communicate with them effectively and better understand how to complete the project. You will be able to determine what information will most help you meet the project needs.

Your documents don’t have to perfectly match these exemplars. The goal is to properly organize the information into the correct fields and understand how planning documents help you prepare for a BI project. This is an opportunity for you to check your understanding, ensure that you’ve met the activity’s expectations, and explore a possible solution.

In the Stakeholder Requirements Document exemplar, you’ll notice that areas where you’re missing information are highlighted in bold. In your own projects, you might find it helpful to note what you’re missing directly on the document. Then, when you ask follow-up questions, you can replace those notes with the correct information.

Most of the information in these docs is pulled directly from the notes in the activity’s scenario. You might also reword something to summarize or expand on notes from the meeting. For example, the notes include “The goal is to understand how these buyers and sellers use their platform. The insights could then inform new-product design and improve the platform.” You might translate this into the business problem section of the planning doc by wording it as “How do buyers and sellers use MarkIt’s platform? How can MarkIt improve their platform?” This frames the language in the notes as a question instead of a statement.

In the Project Requirements Document exemplar, you need to assign each of the stakeholder’s requests as “required,” “desired,” or “nice to have.” A hint for what belongs where can be found in the language the stakeholders use. In the scenario notes, some of the requests might have “must,” while others might have “can we?” or “should.” 

You’ll also find that this document has more fields that are missing information. This can provide clues about what you should ask in follow-up meetings.

The Strategy Document exemplar is the longest document and has more missing information. But because this is the stage where you can determine what kinds of charts you might make, many of your answers can be left to your best judgment. Unless the stakeholder tells you what kinds of charts they want, you will need to use your BI expertise to decide what works best. In this scenario, the stakeholders didn’t give you any chart type suggestions. This means you can make the decisions yourself or decide to follow up with the stakeholder for more guidance. 

The Strategy Document is also where you might include a sketch of a mockup. 



Business Intelligence (BI), its history, and essential tools, comparing the learning journey to an exciting train ride.

BI: A Journey Through Time and Tools

The concept of BI, using data for informed decision-making, has existed for centuries, with historical examples like Sir Henry Furnace's success through data analysis.
The video likens learning BI to a train journey, starting with understanding your current position and desired destination, represented by data models that organize and explain data relationships.
Essential Tools for Your BI Journey

Data pipelines, similar to train tracks, transport data from various sources to a final destination for storage and analysis, often involving transformation processes like ETL (Extract, Transform, Load).
Data visualizations, like memorable photos from a trip, present data graphically using tools like Tableau and Looker, making it accessible and understandable for everyone, and often incorporated into interactive dashboards for monitoring live data.
Iteration: The Key to Continuous Improvement

The video emphasizes the importance of iteration in BI, similar to railway workers constantly improving trains and tracks, involving continuous refinement and innovation of processes for better outcomes.


ETL stands for extract, transform, and load. ETL is a type of data pipeline that enables data to be gathered from source systems, converted into a useful format, and brought into a data warehouse or other unified destination system.




Review technologies and best practices:


Optimal pipeline processes
Developing tools to optimize and automate certain data processes is a large part of a BI professional’s job. Being able to automate processes such as moving and transforming data saves users from having to do that work manually and empowers them with the ability to get answers quickly for themselves. There are a variety of tools that BI professionals use to create pipelines; and although there are some key differences between them, these are many best practices that apply no matter what tool you use. 

Modular design
As you have learned, a data pipeline is a series of processes that transport data from different sources to their final destination for storage and analysis. A pipeline takes multiple processes and combines them into a system that automatically handles the data. Modular design principles can enable the development of individual pieces of a pipeline system so they can be treated as unique building blocks. Modular design also makes it possible to optimize and change individual components of a system without disrupting the rest of the pipeline. In addition, it helps users isolate and troubleshoot errors quickly. 

Other best practices related to modular design include using version control to track changes over time and undo any as needed. Also, BI professionals can create a separate development environment to test and review changes before implementing them.  

Other general software development best practices are also applicable to data pipelines


Verify data accuracy and integrity
The BI processes that move, transform, and report data findings for analysis are only useful if the data itself is accurate. Stakeholders need to be able to depend on the data they are accessing in order to make key business decisions. It’s also possible that incomplete or inaccurate data can cause errors within a pipeline system. Because of this, it’s necessary to ensure the accuracy and integrity of the data, no matter what tools you are using to construct the system. Some important things to consider about the data in your pipelines are: 

Completeness: Is the data complete?

Consistency: Are data values consistent across datasets?

Conformity: Do data values conform to the required format?

Accuracy: Do data values accurately represent actual values?

Redundancy: Are data values redundant within the same dataset?

Integrity: Are data values missing important relationships?

Timeliness: Is the data current?


Creating a testing environment
Building the pipeline processes is only one aspect of creating data pipelines; it’s an iterative process that might require you to make updates and changes depending on how technology or business needs change. Because you will want to continue making improvements to the system, you need to create ways to test any changes before they’re implemented to avoid disrupting users’ access to the data. This could include creating a separate staging environment for data where you can run tests or including a stable dataset that you can make changes to and compare to current processes without interrupting the current flow. 


Dynamic dashboards
Dashboards are powerful visual tools that help BI professionals empower stakeholders with data insights they can access and use when they need them. Dashboards track, analyze, and visualize data in order to answer questions and solve problems



Effective visualizations:


Frameworks for organizing your thoughts about visualization

Frameworks can help you organize your thoughts about data visualization and give you a useful checklist to reference. Here are two frameworks that may be useful for you as you create your own data visualizations: 

The McCandless Method

Kaiser Fung’s Junk Charts Trifecta Checkup


Pre-attentive attributes: marks and channels
Creating effective visuals involves considering how the brain works, then using specific visual elements to communicate the information effectively. Pre-attentive attributes are the elements of a data visualization that people recognize automatically without conscious effort. The essential, basic building blocks that make visuals immediately understandable are called marks and channels. 

Design principles
Once you understand the pre-attentive attributes of data visualization, you can go on to design principles for creating effective visuals. These design principles are vital to your work as a data analyst because they help you make sure that you are creating visualizations that convey your data effectively to your audience. By keeping these rules in mind, you can plan and evaluate your data visualizations to decide if they are working for you and your goals. And, if they aren’t, you can adjust them! 

Avoiding misleading or deceptive charts 
As you have been learning, BI provides people with insights and knowledge they can use to make decisions. So, it’s important that the visualizations you create are communicating your data accurately and truthfully.


Make your visualizations accessible and useful to everyone in your audience by keeping in mind the following:

Labeling

Text alternatives

Text-based format

Distinguishing

Simplifying

As a BI professional, you will encounter a variety of tools for creating pipeline systems, developing dashboards to share with stakeholders, and creating effective visualizations to demonstrate your findings. Those tools require different skills, which take time and effort to learn. But often, you can apply your knowledge to numerous processes and systems.



Business intelligence strategy involves managing the people, processes, and tools used in the business intelligence process.

A data pipeline is a series of processes that transports data from different sources to a new destination for storage and analysis.

This situation describes iteration. Iteration involves repeating a procedure over and over again in order to keep getting closer to the desired result. 

Application programming interface (API): A set of functions and procedures that integrate computer programs, forming a connection that enables them to communicate 

Business intelligence (BI): Automating processes and information channels in order to transform relevant data into actionable insights that are easily available to decision-makers

Business intelligence governance: A process for defining and implementing business intelligence systems and frameworks within an organization

Business intelligence stages: The sequence of stages that determine both BI business value and organizational data maturity, which are capture, analyze, and monitor

Business intelligence strategy: The management of the people, processes, and tools used in the business intelligence process

Data analysts: People who collect, transform, and organize data

Data governance professionals: People who are responsible for the formal management of an organization’s data assets

Data maturity: The extent to which an organization is able to effectively use its data in order to extract actionable insights

Data model: A tool for organizing data elements and how they relate to one another

Data pipeline: A series of processes that transports data from different sources to their final destination for storage and analysis

Data warehousing specialists: People who develop processes and procedures to effectively store and organize data

ETL (extract, transform, and load): A type of data pipeline that enables data to be gathered from source systems, converted into a useful format, and brought into a data warehouse or other unified destination system

Information technology professionals: People who test, install, repair, upgrade, and maintain hardware and software solutions

Iteration: Repeating a procedure over and over again in order to keep getting closer to the desired result

Key performance indicator (KPI): A quantifiable value, closely linked to business strategy, which is used to track progress toward a goal

Portfolio: A collection of materials that can be shared with potential employers

Project manager: A person who handles a project’s day-to-day steps, scope, schedule, budget, and resources

----------------------------------------------------------------------------
Module 2: Business intelligence tools and techniques

This involves learning how to effectively engage with stakeholders, using BI tools to make the most of available data, and applying the power of rapid monitoring to make smart business decisions. In addition, you will start building some career resources by enhancing your online presence, developing strategies for networking and mentorship, and creating a portfolio that will impress future hiring managers. 

the importance of understanding stakeholders in a Business Intelligence project and their roles in the process.

Importance of Stakeholders

BI professionals need to understand their stakeholders' roles, responsibilities, and business goals to tailor BI insights to their specific needs.
Effective communication and teamwork with stakeholders are crucial for aligning project outputs with the team's requirements.
Key Stakeholder Roles

Project Sponsor: Responsible for overall project accountability, setting success criteria, and advocating for the project.
Developer: Creates, executes, tests, and troubleshoots software applications, focusing on either application or system software development.
Systems Analyst: Identifies ways to improve information systems to achieve business goals, often working with raw data to prepare it for BI use.
Business Stakeholder: Invests time and resources in a project and has a vested interest in its outcome, requiring clear communication and collaboration.


Know your stakeholders and their goals
Previously, you learned about the four different types of stakeholders you might encounter as a business intelligence professional: 

Project sponsor: A person who provides support and resources for a project and is accountable for enabling its success.

Developer: A person who uses programming languages to create, execute, test, and troubleshoot software applications. This includes application software developers and systems software developers.

Systems analyst: A person who identifies ways to design, implement, and advance information systems in order to ensure that they help make it possible to achieve business goals.

Business stakeholders: Business stakeholders can include one or more of the following groups of people: 

The executive team: The executive team provides strategic and operational leadership to the company. They set goals, develop strategy, and make sure that strategy is executed effectively. The executive team might include vice presidents, the chief marketing officer, and senior-level professionals who help plan and direct the company’s work. 

The customer-facing team: The customer-facing team includes anyone in an organization who has some level of interaction with customers and potential customers. Typically they compile information, set expectations, and communicate customer feedback to other parts of the internal organization. 

The data science team: The data science team explores the data that’s already out there and finds patterns and insights that data scientists can use to uncover future trends with machine learning. This includes data analysts, data scientists, and data engineers. 

The stakeholders and their goals
Project sponsor
A project sponsor is the person who provides support and resources for a project and is accountable for enabling its success. In this case, the project sponsor is the team lead for the customer-facing team. You know from your discussions with this team that they are interested in optimizing the e-reading app. In order to do so, they need a system that will deliver customer data about purchases and reading time to a database for their analysts to work with. The analysts can then use this data to gain insights about purchasing habits and reading times in order to find out what genres are most popular, how long readers are using the app, and how often they are buying new books to make recommendations to the UI design team. 

Developers
The developers are the people who use programming languages to create, execute, test, and troubleshoot software applications. This includes application software developers and systems software developers. If your new BI workflow includes software applications and tools, or you are going to need to create new tools, then you’ll need to collaborate with the developers. Their goal is to create and manage your business’s software tools, so they need to understand what tools you plan to use and what you need those tools to do. For this example, the developers you work with will be the ones responsible for managing the data captured on the e-reading app.

Systems analyst
The systems analyst identifies ways to design, implement, and advance information systems in order to ensure that they help make it possible to achieve business goals. Their primary goal is to understand how the business is using its computer hardware and software, cloud services, and related technologies, then they figure out how to improve these tools. So the system analyst will be ensuring that the data captured by the developers can be accessed internally as raw data. 

Business stakeholders
In addition to the customer-facing team, who is the project sponsor for this project, there may also be other business stakeholders for this project such as project managers, senior-level professionals, and other executives. These stakeholders are interested in guiding business strategy for the entire business; their goal is to continue to improve business processes, increase revenue, and reach company goals. So your work may even reach the chief technology officer! These are generally people who need bigger-picture insights that will help them make larger scale decisions as opposed to detail-oriented insights about software tools or data systems. 

Conclusion
Often, BI projects encompass a lot of teams and stakeholders who have different goals depending on their function within the organization. Understanding their perspectives is important because it enables you to consider a variety of use cases for your BI tools. And the more useful your tools, the more impactful they will be!


essential communication strategies Business Intelligence (BI) professionals use when collaborating with stakeholders.

Understanding the Needs of Stakeholders

Clearly defining deliverables, which are the products, services, or outcomes needed to complete a project, is crucial for BI project success.
Asking effective questions is key to understanding stakeholder needs, especially when their requests differ from what their actual needs might be.
Defining and Delivering on Expectations

Common BI deliverables include dashboards and reports that offer insights to users, and brainstorming these deliverables should involve considering the problems, challenges, and opportunities at hand.
Visualizing workflows and creating mockups of dashboards can help BI professionals ensure they are meeting stakeholder expectations.
Sharing Insights Effectively

BI professionals must be able to present complex data in a clear and concise way so decision-makers can understand and act on the insights.
Considering bias and ensuring fairness in data interpretation and presentation is an essential responsibility for BI professionals.



Best practices for communicating with stakeholders


Make BI accessible to stakeholders
So far, you have learned three key strategies for communication: 

Ask the right questions 

Define project deliverables

Effectively share business intelligence

Sharing business intelligence can be complicated; you have to be able to simplify technical processes to make them feel straightforward and accessible to a variety of users who might not already understand the terms or concepts. Being able to present intelligence clearly and concisely is critical to making sure that stakeholders can actually use the systems you have created and act on those insights.

There are a few questions you can keep in mind to help guide your communications with stakeholders and partners:   

Who is your audience? When communicating with stakeholders and project partners, it’s important to consider who you’re working with. Consider all of the people who need to understand the BI tools and processes you build when communicating. The sales or marketing team has different goals and expertise than the data science team, for example. 

What do they already know? Because different users have different levels of knowledge and expertise, it can be useful to consider what they already know before communicating with them. This provides a baseline for your communications and prevents you from overexplaining yourself or skipping over any information they need to know.

What do they need to know? Different stakeholders need different kinds of information. For instance, a user might want to understand how to access and use the data or any dashboards you create, but they probably aren’t as interested in the nitty-gritty details about how the data was cleaned. 

How can you best communicate what they need to know? After you have considered your audience, what they already know, and what they need to know, you need to choose the best way to communicate that information to them. This might be an email report, a small meeting, or a cross-team presentation with a Q&A section. 


Create realistic deadlines. Before you start a project, make a list of dependencies and potential roadblocks so you can assess how much extra time to give yourself when you discuss project expectations and timelines with your stakeholders.

Know your project. When you have a good understanding about why you are building a new BI tool, it can help you connect your work with larger initiatives and add meaning to the project. Keep track of your discussions about the project over email or meeting notes, and be ready to answer questions about how certain aspects are important for your organization. In short, it should be easy to understand and explain the value the project is bringing to the company. 

Communicate often. Your stakeholders will want regular updates. Keep track of key project milestones, setbacks, and changes. Another great resource to use is a changelog, which can provide a chronologically ordered list of modifications. Then, use your notes to create a report in a document that you share with your stakeholders. 


Business Intelligence in addressing practical business challenges, like e-commerce cart abandonment, by using data-driven insights and strategic decision-making.

Understanding Cart Abandonment

BI professionals investigate why customers abandon online shopping carts and use data to optimize the customer journey.
They might analyze website loading speeds as a metric, aiming to improve it if delays are causing customers to leave.
Metrics and KPIs in BI

Metrics, like website loading speed, are single, quantifiable data points used to assess performance.
KPIs (Key Performance Indicators) are specific metrics tied to business strategies, such as average transaction value or customer retention rate.
BI Monitoring for Actionable Insights

BI monitoring uses tools for real-time or near real-time data analysis, enabling quick responses to emerging trends or issues.
This allows businesses to promptly address problems like sudden increases in cart abandonment or inventory shortages.



How companies benefit from near-real-time intelligence:

BI monitoring provides businesses with the tools they need to rapidly analyze data and draw insights from continuously updating information. This enables more efficient and impactful decisions, as well as innovation and problem-solving. BI professionals are the key to unlocking these benefits for businesses, which is one of the reasons they’re in such high demand! As you continue through this program, you’ll learn more about BI monitoring and how to turn near-real time data into actionable insights for your stakeholders. 


This situation describes monitoring. Business intelligence monitoring involves using hardware and software tools to rapidly analyze data and communicate key insights, which enable stakeholders to make efficient and impactful business decisions.



A key performance indicator is a quantifiable value, closely linked to business strategy, which is used to track progress toward a goal.

A metric is a single, quantifiable data point that is used to evaluate performance.





----------------------------------------------------------------------------
Module 3: Context is crucial for purposeful insights

you will reexamine the data limitation of context from a BI perspective. Then, you will learn about some other data limitations, including how to address constant change and access insights in a timely manner. You will also discover strategies that BI professionals use to anticipate and overcome these limitations. Finally, you will learn more about metrics and how they relate to context.



Why context is critical:
Context is the who, what, where, when, and why surrounding data that makes it meaningful. Knowing this background information helps us interpret data correctly and visualize useful business intelligence insights for stakeholders. When BI professionals understand the context, choose the right data, and build contextualized visuals to share with stakeholders, they can empower businesses and leadership to make successful decisions.



Data visibility involves the degree or extent to which information can be identified, monitored, and integrated from internal and external sources.



Data ethics and the importance of data privacy:
Likewise, as a BI professional, you have a responsibility to treat data ethically. Data ethics refers to well-founded standards of right and wrong that dictate how data is collected, shared, and used. Throughout your career you will work with a lot of data. This sometimes includes PII, or personally identifiable information, which can be used by itself or with other data to track down a person's identity. One element of treating that data ethically is ensuring that the privacy and security of that data is maintained throughout its lifetime.


Privacy matters
Data privacy means preserving a data subject’s information and activity any time a data transaction occurs. This is also called information privacy or data protection. Data privacy is concerned with the access, use, and collection of personal data. For the people whose data is being collected, this means they have the right to:

Protection from unauthorized access to their private data

Freedom from inappropriate use of their data

The right to inspect, update, or correct their data

Ability to give consent to data collection

Legal right to access the data



Protecting privacy with data anonymization :
Organizations use a lot of different measures to protect the privacy of their data subjects, like incorporating access permissions to ensure that only the people who are supposed to access that information can do so. Another key strategy to maintaining privacy is data anonymization.

Data anonymization is the process of protecting people's private or sensitive data by eliminating PII. Typically, data anonymization involves blanking, hashing, or masking personal information, often by using fixed-length codes to represent data columns, or hiding data with altered values.

Data anonymization is used in just about every industry. As a BI professional, you probably won’t personally be performing anonymization, but it’s useful to understand what kinds of data are often anonymized before you start working with it.



Anticipate data limitations:


Factors of data availability
Previously, you learned about the importance of data availability, which is the degree or extent to which timely and relevant information is readily accessible and able to be put to use. The factors that influence data availability are:

Data integrity: The accuracy, completeness, consistency, and trustworthiness of data throughout its life cycle.

Data visibility: The degree or extent to which information can be identified, monitored, and integrated from disparate internal and external sources.

Update frequency: How often disparate data sources are being refreshed with new information.

Change: The process of altering data, either through internal processes or external influence.

Next, you are going to consider the limitations of data that might change the availability and how you can anticipate those limitations as a BI professional.

Missing data
If you have incomplete or nonexistent data, you might not have enough data to reach a conclusion. Or, you might even be exploring data about a totally different business problem! Understanding what data is available, identifying potential other sources, and filling in the gaps is an important part of the BI process.

Misaligned data
As a BI professional, you will often use data from different sources. Some of these might be internal sources to the business you’re working with, but they might also include external sources. These sources might define and measure things in completely different ways. In cases like these, establishing how to measure things early on standardizes the data across the board for greater reliability and accuracy. This will make sure comparisons between sources are meaningful and insightful.

Dirty data
Dirty data refers to data that contains errors. Dirty data can cause errors in your system, inaccurate reports, and poor decision-making. Implementing processes for cleaning data by fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset is one way you can prepare for this limitation.

Conclusion
As a BI professional, you’ll need to understand that sometimes the data you work with will have limitations. This could mean that it doesn’t fit within a certain time range, or it only applies to specific situations, or there are challenges identifying the data you need. Being able to anticipate those issues and consider them when you build tools and systems for your business will allow you to ensure that those limitations don’t stop your stakeholders from getting the data they need to make great decisions and ensure project success!


concept of vanity metrics in business intelligence and explains how to select effective metrics for dashboards.

Identifying Meaningful Metrics

Vanity metrics may seem impressive but don't offer useful business insights.
Focus on metrics that provide actionable information, like customer loyalty or inventory levels.
Effective Dashboard Design

Prioritize clarity by including only essential metrics relevant to project success.
Align metrics with specific business objectives to measure progress and support decision-making.
Key Considerations for Metric Selection

Use the SMART methodology to ensure metrics are specific, measurable, actionable, relevant, and time-bound.
Display the most critical metric prominently, followed by supporting metrics that provide further detail.



How to identify key metrics for a project:

There are five key points BI professionals take into account when choosing metrics:

1. The number of metrics: More information is not always better. BI professionals limit the number of metrics on dashboards to focus specifically on the ones that are key to a project’s success. Key metrics are relevant and actionable. For instance, if metric X drops, is this good or bad? What action would a user take if it dropped that would be different if it rose instead? Too many metrics that aren’t relevant to the project can be confusing and make your dashboard less effective. The goal isn’t to overload the dashboard to account for every single use case, but 80% of the common use cases.

2. Alignment with business objectives: Understanding the business objectives can help you narrow down which metrics will support those goals and measure their success. For example, if the business objective is to increase sales, include revenue in your dashboard. You will most likely not want to include a metric such as customer satisfaction because that is not directly related to the business objective of increasing sales.

3. The necessary technologies and processes: It’s important to confirm that the necessary technologies and processes are in place for the metrics you’re choosing. If you can’t obtain and analyze the necessary data, then those metrics aren’t going to be very useful.

4. The cadence of data: You have to consider how frequently the data becomes available. If a lot of metrics are delivered at a different cadence and frequency, it becomes difficult to schedule a review.

5. Use SMART methodology: If you earned your Google Data Analytics Certificate, you know the SMART methodology is a useful tool for creating effective questions to ask stakeholders. It can also be used to identify and refine key metrics by ensuring that they are specific, measurable, action-oriented, relevant, and time-bound. This can help you avoid vague or super-high-level metrics that aren’t useful to stakeholders, and instead create metrics that are precise and informative.



The ability to choose metrics that inform decision-making and support project success is a key skill for your career as a BI professional. Remember to consider the number of metrics, how they align with your business objectives, the technologies and processes necessary to measure them, and how they adhere to SMART methodology. It’s also important to maintain an integrated view of the entire business and how the information your metrics deliver is used to guide stakeholder action.




North star metrics:

BI professionals also use another specific kind of metric to measure the long-term success of the entire business or team; this metric is often referred to as a north star metric.

A company’s north star metric goes beyond short-term goals– it’s intended to capture the core measurable value of a business’s product or services over its entire lifetime. These metrics are a guiding light that drive a business forward. That’s why it’s called a north star metric– like the north star can be used to navigate the wilderness, these metrics can be used to navigate business decisions and lead a business to growth.

Having this metric as the guiding light for the entire business is useful in three primary ways:

1. Cross-team alignment: Different teams have different specialties and focuses that help a business function. They aren’t always working on the same projects or with the same metrics, which can make it difficult to align across the entire business. A north star metric allows all of the teams to have a consistent goal to focus on, even as they work on different things.

2. Tracking growth: It can be difficult to understand and track the growth of an entire organization over time without understanding the driving metrics that determine growth. A north star metric provides a long-term measurable data point that stakeholders can focus on when discussing overall performance and growth in a business.

3. Focusing values: A north star metric is primarily a guiding principle for a business– it determines what is important to the organization and stakeholders. This means that choosing the right metric to guide a business can help keep the values in check– whether that’s customer satisfaction, number of customers completing the sales cycle, or customer retention.

Choosing a north star metric
Because north star metrics are so key to a business’s ongoing success, choosing the right metric is a foundational part of a business intelligence strategy. The north star metric has to measure the most essential part or mission of the business. And because every business is different, every business’s north star metric is going to be unique. In order to determine what the most useful north star metric might be, there are a few questions you can ask:

What is essential to this business’s processes?

What are the most important KPIs being measured?

Out of those KPIs, what captures all of the necessary information about this business?

How can the other metrics be structured around that primary metric?




Because more businesses have begun using north star metrics to guide their business strategies, there are a lot of examples of north star metrics in different industries:

E-commerce:

Weekly number of customers completing the sales cycle

Value of daily purchases


As a BI professional, one of your responsibilities will be to empower stakeholders to make business decisions that will promote growth and success over the long term. North star metrics are a great way to measure and guide a business into the future because they allow you to actually measure the success of the entire business, align teams with a single goal, and keep the business’s values at the forefront of their strategy.




Bridge the gap from current state to ideal state:
Gap analysis involves understanding where you currently are compared to where you want to be so that you can bridge the gap. BI uses gap analysis to do all kinds of things, such as improve data delivery systems or create dashboard reports.

For example, perhaps a sales team uses a dashboard to track sales pipeline progress that has a six-hour data lag. They use this dashboard to gather the most up-to-date information as they prepare for important meetings. The six-hour lag is preventing them from accessing and sharing near-real-time insights in stakeholder meetings. Ideally, the delay should be one hour or less.



Setting direction with stakeholders
The first step in bridging the gap is to work with stakeholders to determine the right direction for this BI project. Establishing stakeholder needs and understanding how users are interacting with the data are important for assessing what the ideal state of a system actually is. What needs do stakeholders have that aren’t being met or could be addressed more efficiently? What data is necessary for their decision-making processes? Working closely with stakeholders is necessary to understand what they actually need their BI tools to do.


Context and data quality
In addition to identifying stakeholder needs, it’s also important for the BI professional to understand the context of the data they interact with and present. As you know, context is the condition in which something exists or happens; it turns raw data into meaningful information by providing the data perspective. This involves defining who collected it or funded its collection; the motivation behind that action; where the data came from; when; the method used to collect it; and what the data could have an impact on. BI professionals also need to consider context when creating tools for users to ensure that stakeholders are able to interpret findings correctly and act on them.

It’s also critical that BI professionals ensure the quality and integrity of the data stakeholders are accessing. If the data is incorrect, the reporting tools won’t be accurate, and stakeholders won’t be able to make appropriate decisions — no matter how much context they have been given.


Building structures and systems
A large part of a BI professional’s job is building structures and systems. This means designing database storage systems, organizing the data, and working with database governance specialists to maintain those systems. It also involves creating pipeline tools that move and transform data automatically throughout the system to get data where it needs to go to be useful.

These structures and systems can keep data organized, accessible, and useful for stakeholders during their decision-making process. This empowers users to access the data they need when they need it — an ideal system should be organized and structured to do just that. To address the sales team’s needs, the BI analyst in this case designs a new workflow through which data sources can be processed simultaneously, cutting down processing time from 6 hours to less than an hour.


Acting on insights
BI focuses on automating processes and information channels in order to transform relevant data into actionable insights that are easily available to decision-makers. These insights guide business decisions and development. But the BI process doesn’t stop there: BI professionals continue to measure those results, monitor data, and make adjustments to the system in order to account for changes or new requests from stakeholders.


A large part of a BI professional's work revolves around identifying how current systems and processes operate, evaluating potential improvements, and implementing them so that the current system is closer to the ideal system state. Throughout this course, you’ll learn how to do that by collaborating with stakeholders, understanding context, maintaining data quality, sharing findings, and acting on insights.


Understanding how often machines are serviced can help the VP identify areas for improvement.


Vanity metrics are measures intended to impress others, but don't reflect actual performance. Therefore, they cannot reveal meaningful business insights.

Foot traffic is a vanity metric because a high number of visitors doesn't necessarily mean the campaign was successful.

Employee productivity metrics should revolve around finished tasks.

Project completion is a useful way to measure employee productivity.


Businesses with long wait times will have difficulty attracting and keeping drivers.

Data availability: The degree or extent to which timely and relevant information is readily accessible and able to be put to use

Data integrity: The accuracy, completeness, consistency, and trustworthiness of data throughout its life cycle

Data visibility: The degree or extent to which information can be identified, monitored, and integrated from disparate internal and external sources

Vanity metric: Data points that are intended to impress others, but are not indicative of actual performance and, therefore, cannot reveal any meaningful business insights

Terms and their definitions from previous modules
A
Application programming interface (API): A set of functions and procedures that integrate computer programs, forming a connection that enables them to communicate 

Applications software developer: A person who designs computer or mobile applications, generally for consumers

B
Business intelligence (BI): Automating processes and information channels in order to transform relevant data into actionable insights that are easily available to decision-makers

Business intelligence governance: A process for defining and implementing business intelligence systems and frameworks within an organization

Business intelligence monitoring: Building and using hardware and software tools to easily and rapidly analyze data and enable stakeholders to make impactful business decisions

Business intelligence stages: The sequence of stages that determine both BI business value and organizational data maturity, which are capture, analyze, and monitor

Business intelligence strategy: The management of the people, processes, and tools used in the business intelligence process

D
Data analysts: People who collect, transform, and organize data

Data governance professionals: People who are responsible for the formal management of an organization’s data assets

Data maturity: The extent to which an organization is able to effectively use its data in order to extract actionable insights

Data model: A tool for organizing data elements and how they relate to one another

Data pipeline: A series of processes that transports data from different sources to their final destination for storage and analysis

Data warehousing specialists: People who develop processes and procedures to effectively store and organize data

Deliverable: Any product, service, or result that must be achieved in order to complete a project

Developer: A person who uses programming languages to create, execute, test, and troubleshoot software applications

E
ETL (extract, transform, and load): A type of data pipeline that enables data to be gathered from source systems, converted into a useful format, and brought into a data warehouse or other unified destination system

I
Information technology professionals: People who test, install, repair, upgrade, and maintain hardware and software solutions

Iteration: Repeating a procedure over and over again in order to keep getting closer to the desired result

K
Key performance indicator (KPI): A quantifiable value, closely linked to business strategy, which is used to track progress toward a goal

M
Metric: A single, quantifiable data point that is used to evaluate performance

P
Portfolio: A collection of materials that can be shared with potential employers

Project manager: A person who handles a project’s day-to-day steps, scope, schedule, budget, and resources

Project sponsor: A person who has overall accountability for a project and establishes the criteria for its success

S
Strategy: A plan for achieving a goal or arriving at a desired future state

Systems analyst: A person who identifies ways to design, implement, and advance information systems in order to ensure that they help make it possible to achieve business goals

Systems software developer: A person who develops applications and programs for the backend processing systems used in organizations

T
Tactic: A method used to enable an accomplishment


Vanity metrics are metrics that look good on the surface but don't provide any real insight or value to the business. They are often used to impress or make the organization look good, rather than to drive meaningful decisions. Examples of vanity metrics include the number of social media followers, page views, or website traffic. These metrics may not necessarily correlate with business outcomes or goals.

----------------------------------------------------------------------------
Module 4: Course 1 end-of-course project

you’ll complete a BI portfolio project based on a BI case study. This experiential learning opportunity will enable you to discover how organizations use BI every day and bring together everything you’ve learned about BI in a compelling and instructive way.


Design effective executive summaries

Business intelligence professionals need ways to share and communicate plans, updates, and summaries about projects. A common document called an executive summary is used to update decision makers who may not be directly involved in the tasks of a project. In your role as a BI professional, you will often be involved in creating executive summaries. 

Additionally, an executive summary can be a useful way to describe your end-of-course project to potential employers. This document can give interviewers exploring your portfolio an easy-to-understand explanation of your projects and be a useful way to reference your projects during the actual interview. 


Executive summaries
Executive summaries are documents that collect the most important points contained in a longer plan or report. These summaries are common across a wide variety of businesses, giving decision makers a brief overview of the most relevant information. They can also be used to help new team members become acquainted with the details of a project quickly. The format is designed to respect the responsibilities of decision makers and/or executives who may not have time to read and understand an entire report. There are many ways to present information within an executive summary, including software options built specifically for that purpose. In this program, you will be focusing primarily on a one page format within a presentation slide. Regardless of how they are created, there are some items that are commonly included.


Project title: A project's theme is incorporated into the executive summary title to create an immediate connection with the target audience.

The problem: A statement that focuses on the need or concern being targeted or addressed by the project. Note, also, that the problem can also be referred to as the hypothesis that you’re trying to prove through analysis. 

The solution: This statement summarizes a project’s main goal. In this section, actions are described that are intended to address the concerns outlined in the problem statement.

Details/Key insights: The purpose of this section is to provide any additional background and information that may assist the target audience in understanding the project's objectives. Determining what details to include depends heavily on the intended audience. It may also be the case that you choose to include some project reflections. 



Course 1 workplace scenario overview: Cyclistic:


Welcome to Cyclistic! 
Congrats on your new job with the business intelligence team at Cyclistic, a fictional bike-share company in New York City. In order to provide your team with both BI business value and organizational data maturity, you will use your knowledge of the BI stages: capture, analyze, and monitor. By the time you are done, you will have an end-of-course project that demonstrates your knowledge and skills to potential employers.


Your meeting notes
You recently attended a meeting with key stakeholders to gather details about this BI project. The following details are your notes from the meeting.


Project background:

Cyclistic has partnered with the city of New York to provide shared bikes. Currently, there are bike stations located throughout Manhattan and neighboring boroughs. Customers are able to rent bikes for easy travel between stations at these locations.

Cyclistic’s Customer Growth Team is creating a business plan for next year. The team wants to understand how their customers are using their bikes; their top priority is identifying customer demand at different station locations.

Cyclistic has captured data points for every trip taken by their customers, including:

Trip start time and location (station number, and its latitude/longitude)

Trip end time and location (station number, and its latitude/longitude)

The rented bike’s identification number

The type of customer (either a one-time customer, or a subscriber)




Stakeholders: 

Sara Romero, VP, Marketing

Ernest Cox, VP,  Product Development

Jamal Harris, Director, Customer Data

Nina Locklear, Director, Procurement



Team members: 

Adhira Patel, API Strategist

Megan Pirato, Data Warehousing Specialist

Rick Andersson, Manager, Data Governance 

Tessa Blackwell, Data Analyst

Brianne Sand, Director, IT

Shareefah Hakimi, Project Manager

*Primary contacts are Adhira, Megan, Rick, and Tessa. 




Per Sara: Dashboard needs to be accessible, with large print and text-to-speech alternatives.

Project approvals and dependencies:

The datasets will include customer (user) data, which Jamal will need to approve. Also the project might need approval by the teams that own specific product data, including bike trip duration and bike identification numbers. So I need to make sure that stakeholders have data access to all datasets.

Project goal: Grow Cyclistic’s Customer Base

Details from Ms. Romero:

Understand what customers want, what makes a successful product, and how new stations might alleviate demand in different geographical areas.

Understand how the current line of bikes are used.

How can we apply customer usage insights to inform new station growth?

The customer growth team wants to understand how different users (subscribers and non-subscribers) use our bikes. We’ll want to investigate a large group of users to get a fair representation of users across locations and with low- to high-activity levels.

Keep in mind users might use Cyclistic less when the weather is inclement. This should be visible in the dashboard.

   The deliverables and metrics:

A table or map visualization exploring starting and ending station locations, aggregated by location. I can use any location identifier, such as station, zip code, neighborhood, and/or borough. This should show the number of trips at starting locations.




A visualization showing which destination (ending) locations are popular based on the total trip minutes.

Tip: Focus on peak months.

A visualization that focuses on trends from the summer of 2015.

A visualization showing the percent growth in the number of trips year over year.

Gather insights about congestion at stations.

Tip: For each day, use a table calculation to calculate the net of start and ending trips per station. This gives an approximation of whether there are more bikes coming in or out of a station.

Gather insights about the number of trips across all starting and ending locations.

Gather insights about peak usage by time of day, season, and the impact of weather.




Measure success:

Analyze data that spans at least one year to see how seasonality affects usage. Exploring data that spans multiple months will capture peaks and valleys in usage. Evaluate each trip on the number of rides per starting location and per day/month/year to understand trends. For example, do customers use Cyclistic less when it rains? Or does bikeshare demand stay consistent? Does this vary by location and user types (subscribers vs. nonsubscribers)? Use these outcomes to find out more about what impacts customer demand.

People with dashboard-viewing privileges: 

Adhira, Brianne, Ernest, Jamal, Megan, Nina, Rick, Shareefah, Sara, Tessa

Roll-out:

Week 1: Dataset assigned. Initial design for fields and BikeIDs validated to fit the requirements.

Weeks 2–3: SQL and ETL development

Weeks 3–4: Finalize SQL. Dashboard design. 1st draft review with peers.

Weeks 5–6: Dashboard development and testing

Questions:

How were bikes used by our customers?

How can we apply insights from the data generated by trip data?



Activity: Complete the business intelligence project documents for Cyclistic



Step 3: Complete the Stakeholder Requirements Document
Use your meeting notes to fill out as much of the project requirements document template as you can. If you find that there are some fields that you can’t fill out, make note of them for later in the exercise.

The Stakeholder Requirements Document enables you to capture stakeholder requests and requirements so you understand their needs before planning the rest of the project details or strategy. It should answer the following questions:

Business problem: What is the primary question to be answered or problem to be solved?

Stakeholders: Who are the major stakeholders of this project, and what are their job titles? 

Stakeholder usage details: How will the stakeholders use the BI tool?

Primary requirements: What requirements must be met by this BI tool in order for this project to be successful? 

Here are some questions BI professionals ask to successfully complete this document:

What questions must be answered before starting this project? 

What does the BI team need to know before starting this project? 

What are the questions that must be answered or problems that must be solved by this project?

What datasets are considered important to this project?

Who should have access to the dashboard? Will the entire dashboard be visible to all stakeholders?

Typically, the Stakeholder Requirements Document is a one-pager with notes, but it can be longer and more detailed for complex projects. 




Step 4: Complete the Project Requirements Document
Use your meeting notes to fill out as much of the project requirements document template as you can. If you find that there are some fields that you can’t fill out, make note of them for later in the exercise.

The Project Requirements Document contains the following details:

Purpose: Briefly describe why this project is happening and explain why the company should invest its resources in it.

Key dependencies: Detail the major elements of this project. Include the team, primary contacts, and expected deliverables. Are there any inter-team deliverables required? 

Stakeholder requirements: List the established stakeholder requirements, based on the Stakeholder Requirements Document. Prioritize the requirements as: R - required, D - desired, or N - nice to have.

Success criteria: Clarify what success looks like for this project. Include explicit statements about how to measure success. Use SMART criteria. 

User journeys: Document the current user experience and the ideal future experience. 

Assumptions: Explicitly and clearly state any assumptions you are making. 

Compliance and privacy: Include compliance, privacy, or legal dimensions to consider. 

Accessibility: List key considerations for creating accessible reports for all users. Who needs to access this feature? How are they viewing and interacting with it? 

Roll-out plan: Briefly describe the expected scope, priorities and timeline. Consider at what points during the rollout will measurements be made to determine whether the feature is performing as expected? Is there a rollback plan and timeline if this feature does not meet its intended goals?Step 4: Complete the Project Requirements Document
Use your meeting notes to fill out as much of the project requirements document template as you can. If you find that there are some fields that you can’t fill out, make note of them for later in the exercise.

The Project Requirements Document contains the following details:

Purpose: Briefly describe why this project is happening and explain why the company should invest its resources in it.

Key dependencies: Detail the major elements of this project. Include the team, primary contacts, and expected deliverables. Are there any inter-team deliverables required? 

Stakeholder requirements: List the established stakeholder requirements, based on the Stakeholder Requirements Document. Prioritize the requirements as: R - required, D - desired, or N - nice to have.

Success criteria: Clarify what success looks like for this project. Include explicit statements about how to measure success. Use SMART criteria. 

User journeys: Document the current user experience and the ideal future experience. 

Assumptions: Explicitly and clearly state any assumptions you are making. 

Compliance and privacy: Include compliance, privacy, or legal dimensions to consider. 

Accessibility: List key considerations for creating accessible reports for all users. Who needs to access this feature? How are they viewing and interacting with it? 

Roll-out plan: Briefly describe the expected scope, priorities and timeline. Consider at what points during the rollout will measurements be made to determine whether the feature is performing as expected? Is there a rollback plan and timeline if this feature does not meet its intended goals?



Course 1 workplace scenario overview: Google Fiber:


Welcome to Google Fiber! 
You are interviewing for a job with Google Fiber, which provides people and businesses with fiber optic internet. As part of the interview process, the Fiber customer service team has asked you to design a dashboard using fictional data. The position you are interviewing for is in the customer call center, where Fiber uses business intelligence to monitor and improve customer satisfaction.

To provide the interviewers with both BI value and organizational data maturity, you will use your knowledge of the BI stages: capture, analyze, and monitor. By the time you are done, you will have an end-of-course project that demonstrates your knowledge and skills to potential employers.


Your meeting notes
You are interviewing with the Google Fiber customer service team for a position as a BI analyst. At the end of the first interview, you spoke with the BI team and hiring manager to gather details about this project. Following are your notes from the meeting. Use the information they contain to complete the Stakeholder Requirements Document, Project Requirements Document, and Planning Document.



Project background:

The team needs to understand how often customers  phone customer support again after their first inquiry; this will help leaders understand whether the team is able to answer customer questions the first time. Further, leaders want to explore trends in repeat calls to identify why customers are having to call more than once, as well as how to improve the overall customer experience. I will create a dashboard to reveal insights about repeat callers. 

This fictional dataset is a version of actual data the team works with. Because of this, the data is already anonymized and approved. It includes:

Number of calls

Number of repeat calls after first contact

Call type

Market city

Date


Stakeholders: 

Emma Santiago, Hiring Manager

Keith Portone, Project Manager

Minna Rah, Lead BI Analyst

Team members: 

Ian Ortega, BI Analyst

Sylvie Essa, BI Analyst

*Primary contacts are Emma and Keith

Per Minna: Dashboard needs to be accessible, with large print and text-to-speech alternatives.


Project approvals and dependencies:

I need to make sure stakeholders have access to all datasets so they can explore the steps I’ve taken.

Project goal: Explore trends in repeat callers

Details from Mr. Portone:

Understand how often customers are calling customer support after their first inquiry; this will help leaders understand how effectively the team is able to answer customer questions the first time

Provide insights into the types of customer issues that seem to generate more repeat calls

Explore repeat caller trends in the three different market cities

Design charts so that stakeholders can view trends by week, month, quarter, and year. 

   The deliverables and metrics:

A chart or table measuring repeat calls by their first contact date

A chart or table exploring repeat calls by market and problem type

Charts showcasing repeat calls by week, month, and quarter

Measure success:

The team’s ultimate goal is to reduce call volume by increasing customer satisfaction and improving operational optimization. My dashboard should demonstrate an understanding of this goal and provide stakeholders with insights about repeat caller volumes in different markets and the types of problems they represent. 

Other considerations:

In order to anonymize and fictionalize the data, the datasets the columns market_1, market_2, and market_3 to indicate three different city service areas the data represents. 

The data also lists five problem types:

Type_1 is account management

Type_2 is technician troubleshooting

Type_3 is scheduling

Type_4 is construction

Type_5 is internet and wifi

Additionally, the dataset records repeat calls over seven-day periods. The initial contact date is listed as contacts_n. The other call columns are then contacts_n_number of days since first call. For example, contacts_n_6 indicates six days since first contact. 

People with dashboard-viewing privileges: 

Emma Santiago, Keith Portone, Minna Rah, Ian Ortega, Sylvie Essa

Questions:

How often does the customer service team receive repeat calls from customers?

What problem types generate the most repeat calls?

Which market city’s customer service team receives the most repeat calls?


Step 3: Complete the Stakeholder Requirements Document
Use your meeting notes to fill out as much of the project requirements document template as you can. If you find that there are some fields that you can’t fill out, make note of them for later in the exercise.

The Stakeholder Requirements Document enables you to capture stakeholder requests and requirements so you understand their needs before planning the rest of the project details or strategy. It should answer the following questions:

Business problem: What is the primary question to be answered or problem to be solved?

Stakeholders: Who are the major stakeholders of this project, and what are their job titles? 

Stakeholder usage details: How will the stakeholders use the BI tool?

Primary requirements: What requirements must be met by this BI tool in order for this project to be successful? 



Step 4: Complete the Project Requirements Document
Use your meeting notes to fill out as much of the project requirements document template as you can. If you find that there are some fields that you can’t fill out, make note of them for later in the exercise.

The Project Requirements Document contains the following details:

Purpose: Briefly describe why this project is happening and explain why the company should invest its resources in it.

Key dependencies: Detail the major elements of this project. Include the team, primary contacts, and expected deliverables. Are there any inter-team deliverables required? 

Stakeholder requirements: List the established stakeholder requirements, based on the Stakeholder Requirements Document. Prioritize the requirements as: R - required, D - desired, or N - nice to have.

Success criteria: Clarify what success looks like for this project. Include explicit statements about how to measure success. Use SMART criteria. 

User journeys: Document the current user experience and the ideal future experience. 

Assumptions: Explicitly and clearly state any assumptions you are making. 

Compliance and privacy: Include compliance, privacy, or legal dimensions to consider. 

Accessibility: List key considerations for creating accessible reports for all users. Who needs to access this feature? How are they viewing and interacting with it? 

Roll-out plan: Briefly describe the expected scope, priorities and timeline. Consider at what points during the rollout will measurements be made to determine whether the feature is performing as expected? Is there a rollback plan and timeline if this feature does not meet its intended goals?




Dont Forget to access the project:

1. The relevant datasets have been accessed and applied to the project. 

2. Key details from the meeting notes are identified and documented.

3. The primary stakeholders are identified and documented.

4. The relevant business questions are identified and documented.

5. The primary requirements for the final deliverable are identified and documented. 

6. The function of the dashboard—including access, scope, filters, and granularity—is identified and documented.

7. The metrics to be included in the final dashboard are identified and documented.

8. Outlines for a minimum of three charts to be included in the final dashboard are documented. 

9. Information about each chart includes details about the required metrics and dimensions.

10. The project planning documents provide relevant information that prepares the project for next steps.




============================================================================
2. Data Models and Pipelines

business intelligence professionals use data models and schemas to organize and optimize databases

Explore data modeling and ETL processes for extracting data from source systems, transforming it into formats that better enable analysis, and driving business processes and goals.


----------------------------------------------------------------------------
Module 1: Data models and pipelines

The importance of data organization in business intelligence (BI), introduces the concept of data modeling, and explores how design patterns and schemas contribute to building effective data systems.

Data Organization and Systems

- BI professionals often create destination databases and organize data systems, including data lakes, OLTP databases, data marts, and OLAP databases.
- Understanding the difference between structured and unstructured data is crucial for effective data organization.


Data Modeling

- Data modeling helps organize data elements and ensures consistency across systems, acting like a map for navigating databases.
- Design patterns, which are reusable problem-solving templates, are commonly used to create data models tailored to specific business needs.


Schemas and Their Role

- Database schemas describe how data is organized, with common examples including relational models, star schemas, snowflake schemas, and NoSQL schemas.
- Essentially, if a design pattern is the template for a data model, the schema summarizes that model.


In order to create an effective model, a design pattern uses measures and facts that are important to the business.


Dimensional modeling, a technique used in business intelligence to retrieve data quickly from data warehouses.

Understanding Relational Databases

- Relational databases use tables with primary and foreign keys to establish relationships, enabling connections between different tables.
- A primary key uniquely identifies each record in a table, while a foreign key refers to a primary key in another table, linking the tables.

Exploring Dimensional Modeling

- Dimensional models are designed for efficient data retrieval and consist of facts (measurements) and dimensions (contextual information).
- Fact tables store the measurements, while dimension tables hold attributes that provide context to the facts, connecting to fact tables via foreign keys.



This section introduces common database schemas used in Business Intelligence, focusing on their structures and purposes.

Schemas: The Blueprint of Data

- A schema acts as a blueprint, defining how data is organized within a database, including its structure, relationships, and properties.
- It doesn't contain the data itself but provides a framework for understanding and querying the data.

Star and Snowflake Schemas

- Star schemas are simple, efficient structures with a central fact table connected to multiple dimension tables, ideal for quick data retrieval and reporting.
- A star schema consists of one fact table that references any number of dimension tables. 
- Snowflake schemas, an extension of star schemas, introduce subdimensions, creating a more complex but detailed representation of data relationships.

Understanding Schemas in BI

- Familiarity with schemas is crucial for BI professionals as it helps them comprehend database structures and optimize data retrieval for analysis.
- Different schemas serve different analytical needs, and understanding their strengths and limitations is key to effective data modeling and analysis.



How to Design efficient database systems with schemas:

a schema is a way of describing the way something is organized. Think about data schemas like blueprints of how a database is constructed. This is very useful when exploring a new dataset or designing a relational database. A database schema represents any kind of structure that is defined around the data. At the most basic level, it indicates which tables or relations make up the database, as well as the fields included on each table.

Types of Schema:

Jadi setiap database itu pasti ada satu tabel UTAMA yang mewakili seluruh informasi dari seluruh tabel lainnya

1. A star schema is a schema consisting of one or more fact tables referencing any number of dimension tables. As its name suggests, this schema is shaped like a star. This type of schema is ideal for high-scale information delivery and makes read output more efficient. It also classifies attributes into facts and descriptive dimension attributes (product ID, customer name, sale date).

Jadi intinya itu ada satu tabel utama yang berelasi kebanyak tabel lainnya


2. A snowflake schema is an extension of a star schema with additional dimensions and, often, subdimensions. These dimensions and subdimensions create a snowflake pattern. Like snowflakes in nature, a snowflake schema—and the relationships within it—can be complex. Snowflake schemas are an organization type designed for lightning-fast data processing.

Intinya kayak ada ada tabel yang memiliki beberapa relasi, nah di relasi itu ada juga relasinya


3. Flat model
Flattened schemas are extremely simple database systems with a single table in which each record is represented by a single row of data. The rows are separated by a delimiter, like a column, to indicate the separations between records. Flat models are not relational; they can’t capture relationships between tables or data items. Because of this, flat models are more often used as a potential source within a data system to capture less complex data that doesn’t need to be updated.


simpel database tabel cuma punya satu tabel	


4. Semi-structured schemas
In addition to traditional, relational schemas, there are also semi-structured database schemas which have much more flexible rules, but still maintain some organization. Because these databases have less rigid organizational rules, they are extremely flexible and are designed to quickly access data.



5. Document schemas store data as documents, similar to JSON files. These documents store pairs of fields and values of different data types.


6. Key-value schemas pair a string with some relationship to the data, like a filename or a URL, which is then used as a key. This key is connected to the data, which is stored in a single collection. Users directly request data by using the key to retrieve it.

7. Wide-column schemas use flexible, scalable tables. Each row contains a key and related columns stored in a wide format.

8. Graph schemas  store data items in collections called nodes. These nodes are connected by edges, which store information about how the nodes are related. However, unlike relational databases, these relationships change as new data is introduced into the nodes.



As a BI professional, you will often work with data that has been organized and stored in different ways. Different database models and schemas are useful for different things, and knowing that will help you design an efficient database system!



Different data types, different databases:

Database Frameworks and Their Significance

- Different database frameworks are designed for different purposes, such as processing transactions or analyzing data.
- Understanding the type of database your company uses is crucial for designing effective data models and facilitating database migrations.

OLTP vs. OLAP Databases

- OLTP databases are optimized for processing transactions, ensuring data consistency and managing modifications.
- OLAP databases are designed for analyzing data from multiple sources to provide business insights.

- A distributed database framework features a collection of data systems that exist across many physical locations.



Database comparison checklist:


1. OLAP
Online Analytical Processing (OLAP) systems are databases that have been primarily optimized for analysis.

Provide user access to data from a variety of source systems

Used by BI and other data professionals to support decision-making processes

Analyze data from multiple databases

Draw actionable insights from data delivered to reporting tables


2. OLTP
Online Transaction Processing (OLTP) systems are databases that have been optimized for data processing instead of analysis.

Store transaction data

Used by customer-facing employees or customer self-service applications

Read, write, and update single rows of data

Act as source systems that data pipelines can be pulled from for analysis



Row-based versus columnar

1. Row-based
Row-based databases are organized by rows.
Traditional, easy to write database organization typically used in OLTP systems

Writes data very quickly

Stores all of a row’s values together

Easily optimized with indexing

2. Columnar
Columnar databases are organized by columns instead of rows.

Newer form of database organization, typically used to support OLAP systems

Read data more quickly and only pull the necessary data for analysis

Stores multiple row’s columns together


Distributed versus single-homed
1. Distributed
Distributed databases are collections of data systems distributed across multiple physical locations.

Easily expanded to address increasing or larger scale business needs

Accessed from different networks

Easier to secure than a single-homed database system

2. Single-homed
Single-homed databases are databases where all of the data is stored in the same physical location.

Data stored in a single location is easier to access and coordinate cross-team

Cuts down on data redundancy

Cheaper to maintain than larger, more complex systems


Separated storage and compute versus combined
1. Separated storage and compute

Separated storage and computing systems are databases where less relevant data is stored remotely, and relevant data is stored locally for analysis.

Run analytical queries more efficiently because the system only needs to process the most relevant data

Scale computation resources and storage systems separately based on your organization’s custom needs


2. Combined storage and compute

Combined systems are database systems that store and analyze data in the same place.

Traditional setup that allows users to access all possible data at once

Storage and computation resources are linked, so resource management is straightforward


A business intelligence professional stores large amounts of raw data in its original format within a database system. Then, they can access the data whenever they need it for their BI project. What type of database are they using data lake.


The shape of the data:
Data Warehouse Fundamentals:

- A data warehouse is a type of database designed to support data-driven decision-making by providing a unified, consistent view of data from multiple sources.
- Business Intelligence (BI) professionals play a crucial role in designing data warehouses by aligning them with business needs, considering data characteristics, and choosing appropriate data models.

Designing a Data Warehouse:

- Understanding business needs is paramount, as it dictates how data is used, stored, and organized within the warehouse. For example, a hospital tracking patient health trends has different requirements than a financial firm analyzing market data.
- The shape and volume of data from source systems are critical factors. "Shape" refers to the structure of data tables (rows and columns), while "volume" considers the amount of data, both current and projected.

Data Modeling for a Bookstore:

- In a bookstore scenario, understanding sales data is key. A "Sales" fact table might include quantities, amounts, taxes, discounts, and net values.
- Dimension tables like "Store," "Customer," "Product," "Promotion," "Time," "Stock," and "Currency" provide context to the sales data, forming a star schema. This model helps analyze promotion effectiveness and generate insightful reports.



Design useful database schemas:

Importance of a Database Schema

- A database schema is like a blueprint that describes the structure and organization of your data, ensuring everyone understands the data layout.
- It's crucial for maintaining data integrity, especially when adding new information or integrating data from different sources.


Essential Elements of a Schema:
- A functional schema must include all relevant data, clearly labeled columns with specified data types (like organizing kitchen drawers), consistent formatting for all entries to prevent errors, and unique keys (primary and foreign) to establish relationships between tables.
- These elements ensure your schema remains useful and adaptable to evolving business needs.



Four key elements of database schemas:
- The relevant data: The schema describes how the data is modeled and shaped within the database and must encompass all of the data being described.

- Names and data types for each column: Include names and data types for each column in each table within the database.

- Consistent formatting: Ensure consistent formatting across all data entries. Every entry is an instance of the schema, so it needs to be consistent.

- Unique keys: The schema must use unique keys for each entry within the database. These keys build connections between the tables and enable users to combine relevant data from across the entire database.




Data pipelines and the ETL process:

Data Pipelines: A Solution for Efficient Data Management

- Data pipelines automate the flow of data from various sources to a final destination, saving time and resources while making data more accessible.
- They eliminate the need for manual data transfers, which is especially crucial when dealing with large volumes of data, like daily weather information.

Understanding ETL: The Backbone of Data Pipelines

- ETL (Extract, Transform, Load) is a standard process within data pipelines that ensures data is gathered, converted into a usable format, and loaded into a unified system like a data warehouse.
- During the transformation stage, data is cleaned and validated, making it ready for analysis once it reaches its destination.

Benefits of Data Pipelines for BI Professionals

- Data pipelines can be scheduled to run automatically, freeing up BI professionals to focus on analyzing data and extracting valuable insights.
- By automating data-related tasks, BI teams can work more efficiently and dedicate more time to generating business value from data.



Data Pipeline:
1. Ingesting raw data
Raw data is taken from a source system, such as a data lake or a warehouse, before being ingested into the pipeline. This can be a single source or collection of sources for use in the target system.

2. Processing and consolidating the data
While the data is moving through the pipeline, it is transformed to ensure its usefulness for analysts and stakeholders in the future. This could include performing data transformations, data cleaning, and data sorting.

3. Delivering the data
Finally, after data has been taken from the source system and processed through the pipeline, it’s delivered to the destination system. This could include an analytical database system, reporting tables, or dynamic dashboards that keep updated information for stakeholders.



Maximize data through the ETL process:

- ETL stands for Extract, Transform, and Load, which represent the three stages of this data pipeline process.
- ETL pipelines take data from source systems, convert it into a usable format, and then load it into a target system like a data warehouse or data lake.


Stages of an ETL Pipeline:
1. Extraction: Data is read and collected from various source systems, such as transactional databases or flat files.
2. Transformation: The extracted data is cleaned, validated, and transformed into a format suitable for analysis in the target system. This includes mapping data types to match the destination's requirements.
3. Loading: The transformed data is delivered to its final destination, which could be a data warehouse, data lake, or another analytics platform.


In the transform stage, a business intelligence professional maps data types from the sources to the target system in order to ensure the data fits the destination.



Factors to Consider When Choosing BI Tools
1. KPIs: Select tools that align with your organization's Key Performance Indicators (KPIs) to effectively track progress towards goals.
2. Data Visualization: Consider how stakeholders prefer to view data (e.g., graphs, reports, dashboards) and choose tools accordingly.
3. Back-end Integration: Ensure the chosen tools can interact with your data storage solutions, such as data lakes, and support necessary data transfer and transformation processes.



Business intelligence tools and their applications
1. Azure Analysis Service (AAS)

Connect to a variety of data sources

Build in data security protocols

Grant access and assign roles cross-team

Automate basic processes


2. CloudSQL

Connect to existing MySQL, PostgreSQL or SQL Server databases

Automate basic processes

Integrate with existing apps and Google Cloud services, including BigQuery

Observe database processes and make changes


3. Looker Studio

Visualize data with customizable charts and tables

Connect to a variety of data sources

Share insights internally with stakeholders and online

Collaborate cross-team to generate reports

Use report templates to speed up your reporting


4. Microsoft PowerBI

Connect to multiple data sources and develop detailed models

Create personalized reports

Use AI to get fast answers using conversational languages

Collaborate cross-team to generate and share insights on Microsoft applications


5. Pentaho

Develop pipelines with a codeless interface

Connect to live data sources for updated reports

Establish connections to an expanded library

Access an integrated data science toolkit


6. SSAS SQL Server

Access and analyze data across multiple online databases

Integrate with existing Microsoft services including BI and data warehousing tools and SSRS SQL Server

Use built-in reporting tools


7. Tableau

Connect and visualize data quickly

Analyze data without technical programming languages

Connect to a variety of data sources including spreadsheets, databases, and cloud sources

Combine multiple views of the data in intuitive dashboards

Build in live connections with updating data sources



ETL-specific tools and their applications:

1. Apache Nifi

	
Connect a variety of data sources

Access a web-based user interface

Configure and change pipeline systems as needed

Modify data movement through the system at any time


2. DataFlow
Synchronize or replicate data across a variety of data sources

Identify pipeline issues with smart diagnostic features

Use SQL to develop pipelines from the BigQuery UI

Schedule resources to reduce batch processing costs

Use pipeline templates to kickstart the pipeline creation process and share systems across your organization


3. IBM InfoSphere Information Server

Integrate data across multiple systems

Govern and explore available data

Improve business alignment and processes

Analyze and monitor data from multiple data sources


4. Microsoft SQL SIS

Connect data from a variety of sources integration

Use built-in transformation tools

Access graphical tools to create solutions without coding

Generate custom packages to address specific business needs


5. Oracle Data Integrator

Connect data from a variety of sources

Track changes and monitor system performance with built-in features

Access system monitoring and drill-down capabilities

Reduce monitoring costs with access to built-in Oracle services


6. Pentaho Data Integrator

Connect data from a variety of sources

Create codeless pipelines with drag-and-drop interface

Access dataflow templates for easy use

Analyze data with integrated tools


7. Talend

Connect data from a variety of sources

Design, implement, and reuse pipeline from a cloud server

Access and search for data using integrated Talend services

Clean and prepare data with built-in tools


Target tables are the predetermined locations where pipeline data is sent in order to be acted on. 

This describes the extract stage. During extraction, the pipeline accesses source systems, then reads and collects the necessary data from within them. 


Applying knowledge of one tool to another is an example of a transferable skill.


Introduction to Dataflow:

Why Google Dataflow?

- Many data pipeline tools share similar concepts and procedures.
- Skills learned in Google Dataflow are transferable to other tools.

Dataflow Features

- Reads, transforms, and writes data.
- Uses open-source libraries and supports Python and SQL.
- Offers pre-built and customizable templates.
- Includes security features for data protection.

Exploring Dataflow

- Access the jobs page to manage projects.
- Utilize snapshots for version control and testing.
- Explore the pipeline section to enable APIs and set locations.
- Leverage Jupyter Notebooks for examples and visualization.
- Write and execute SQL queries in the SQL workspace.


Dataflow is a serverless data-processing service that reads data from the source, transforms it, and writes it in the destination location.

The Workbench section is where you can create and save shareable Jupyter notebooks with live code. This is helpful for first-time ETL tool users to check out examples and visualize the transformations.



Guide to Dataflow:
As you have been learning, Dataflow is a serverless data-processing service that reads data from the source, transforms it, and writes it in the destination location. Dataflow creates pipelines with open source libraries, with which you can interact using different languages, including Python and SQL. This reading provides information about accessing Dataflow and its functionality. 



Jobs 
When you first open the console, you will find the Jobs page. The Jobs page is where your current jobs are in your project space. There are also options to CREATE JOB FROM TEMPLATE or CREATE MANAGED DATA PIPELINE from this page, so that you can get started on a new project in your Dataflow console. This is where you will go anytime you want to start something new. 

Pipelines
Open the menu pane to navigate through the console and find the other pages in Dataflow. The Pipelines menu contains a list of all the pipelines you have created. If this is your first time using Dataflow, it will also display the processes you need to enable before you can start building pipelines. If you haven’t already enabled the APIs, click Fix All to enable the API features and set your location. 


Workbench 
The Workbench section is where you can create and save shareable Jupyter notebooks with live code. This is helpful for first-time ETL tool users to check out examples and visualize the transformations.


Snapshots
Snapshots save the current state of a pipeline to create new versions without losing the current state. This is useful when you are testing or updating current pipelines so that you aren’t disrupting the system. This feature also allows you to back up and recover old project versions. You may need to enable APIs to view the Snapshots page; you will learn more about APIs in an upcoming activity. 


SQL Workspace
Finally, the SQL Workspace is where you interact with your Dataflow jobs, connect to BigQuery functionality, and write necessary SQL queries for your pipelines.


Data pipelines automate the processes involved in extracting, transforming, combining, validating, and loading data for further analysis and visualization. Effective data pipelines also help eliminate errors and combat system latency.

The Pub/Sub Topic to BigQuery template is a streaming pipeline that reads JSON-formatted messages from a Pub/Sub topic and writes them to a BigQuery table. You can use this template as a quick solution to move Pub/Sub data to BigQuery.


Merge data from multiple sources with BigQuery:
Data extraction
In order to obtain the information the stakeholders are interested in, Aviva begins extracting the data. The data extraction process includes locating and identifying relevant data, then preparing it to be transformed and loaded. To identify the necessary data, Aviva implements the following strategies:

Meet with key stakeholders
Aviva leads a workshop with stakeholders to identify their objectives. During this workshop, she asks stakeholders questions to learn about their needs:

What information needs to be obtained from the data (for instance, performance of different menu items at different restaurant locations)?

What specific metrics should be measured (sales metrics, marketing metrics, product performance metrics)?

What sources of data should be used (sales numbers, customer feedback, point of sales)?

Who needs access to this data (management, market analysts)?

How will key stakeholders use this data (for example, to determine which items to include on upcoming menus, make pricing decisions)?


Observe teams in action
Aviva also spends time observing the stakeholders at work and asking them questions about what they’re doing and why. This helps her connect the goals of the project with the organization’s larger initiatives. During these observations, she asks questions about why certain information and activities are important for the organization.

Organize data in BigQuery
Once Aviva has completed the data extraction process, she transforms the data she’s gathered from different stakeholders and loads it into BigQuery. Then she uses BigQuery to design a target table to organize the data. The target table helps Aviva unify the data. She then uses the target table to develop a final dashboard for stakeholders to review. 


If a query entered into the SQL workspace is valid, then a check mark appears along with the amount of data that the query processes.

The SQL workspace is used to run, save, share, and schedule queries and find resources. 

BigQuery is a data warehouse on the Google Cloud Platform (GCP) used to query, filter large datasets, aggregate results, and perform complex operations.



Unify data with target tables:

Data extraction
Data extraction is the process of taking data from a source system, such as a database or a SaaS, so that it can be delivered to a destination system for analysis. You might recognize this as the first step in an ETL (extract, transform, and load) pipeline. There are three primary ways that pipelines can extract data from a source in order to deliver it to a target table:

Update notification: The source system issues a notification when a record has been updated, which triggers the extraction.

Incremental extraction: The BI system checks for any data that has changed at the source and ingests these updates.

Full extraction: The BI system extracts a whole table into the target database system.

Once data is extracted, it must be loaded into target tables for use. In order to drive intelligent business decisions, users need access to data that is current, clean, and usable. This is why it is important for BI professionals to design target tables that can hold all of the information required to answer business questions.




As a BI professional, you will want to take advantage of target tables as a way to unify your data and make it accessible to users. In order to draw insights from a variety of different sources, having a place that contains all of the data from those sources is essential.

The loading stage of the BI pipeline is when data is delivered to its target destination. Data is pulled from a source and moved into a target table during the extraction stage.




Case study: Wayfair - Working with stakeholders to create a pipeline:

Company background
Longtime friends Niraj Shah and Steve Conine started the online-only company in 2002 after deciding they wanted to offer a larger selection of choices to customers—more than could fit in a brick-and-mortar space. They started the company as a collection of more than 200 e-commerce stores, each selling separate categories of products. In 2011, the company combined these sites to establish wayfair.com.


The challenge
The Wayfair pricing ecosystem includes thousands of different inputs and outputs across a full catalog of products, which change multiple times a day. All of these inputs and outputs are being generated in different ways from different sources. Because of this, the BI team and other data professionals who needed to access pricing data were having trouble locating, querying, and interpreting the complete dataset. This led to incomplete and often inaccurate insights that weren’t useful for decision makers. 

To address this, the BI team decided to design and implement a new pipeline system to consolidate all the data stakeholders needed. They also needed to consider a few additional challenges with their pipeline system:

Monitoring and reporting around these processes would need to be included in the design to track and manage errors.

Data would need to be clean before it could be shared with downstream users. 

Due to the variety of data types being joined, the BI team also needed to better understand the data relationships so they could accurately consolidate the data. 

Training sessions would be required to help educate users on how to best access and use the new datasets. 

The approach 
Given the massive amount of data within the system, it was important for the BI team to step back and work with stakeholders to really understand how they were using the data currently. That included understanding the business problems they were trying to solve, the data they were already using and how they were accessing it, and the data they wanted to use but couldn’t access yet. 

Once they had communicated with stakeholders, the team was able to design a pipeline that achieved three key goals:

All the required data could be made available and easy to understand and use

The system was more efficient and could make data available without delays

The system was designed to scale as the dataset expanded vertically and horizontally to support future growth. This project required collaboration across a variety of stakeholders and teams:

- Software engineers: The software engineer team were the primary owners and generators of data, so they were key to understanding the current state of the data and helped make it accessible for the BI team to work with.

Data architects: The BI team consulted with data architects to ensure that the pipeline design was all-encompassing, efficient, and scalable so the BI team could handle the amount of data being ingested by the system and ensure that downstream users would have access to the data as the system was being scaled. 

Data professionals: As the core users, these teams provided the use cases and requirements for the system so that the BI team could ensure that the pipeline addressed their needs. Because each of their respective teams’ needs were different, it was important to ensure the system design and data included was wide enough to account for all of those needs. 

Business stakeholders: As the end users of the insights generated by the entire pipeline, the business stakeholders ensured all development work and use cases were rooted with clear business problems to ensure what the BI team built could be immediately applied to their work. 


The final pipeline that the BI team implemented achieved a variety of key goals for the entire organization:

It enabled software engineering teams to publish data in real-time for the BI team to use.

It consolidated the different data components into one unified dataset for ease of access and use.

It allowed the BI team to store different data components in their own individual staging layers.

It included additional processes to monitor and report on the system’s performance to inform users where failures were occurring and enable quick fixes.

It created a unified dataset that users could leverage to build metrics and report on data.


Attribute: In a dimensional model, a characteristic or quality used to describe a dimension

Columnar database: A database organized by columns instead of rows

Combined systems: Database systems that store and analyze data in the same place

Compiled programming language: A programming language that compiles coded instructions that are executed directly by the target machine

Data lake: A database system that stores large amounts of raw data in its original format until it’s needed

Data mart: A subject-oriented database that can be a subset of a larger data warehouse

Data warehouse: A specific type of database that consolidates data from multiple source systems for data consistency, accuracy, and efficient access

Database migration: Moving data from one source platform to another target database

Dimension (data modeling): A piece of information that provides more detail and context regarding a fact

Dimension table: The table where the attributes of the dimensions of a fact are stored

Design pattern: A solution that uses relevant measures and facts to create a model in support of business needs

Dimensional model: A type of relational model that has been optimized to quickly retrieve data from a data warehouse

Distributed database: A collection of data systems distributed across multiple physical locations

Fact: In a dimensional model, a measurement or metric

Fact table: A table that contains measurements or metrics related to a particular event

Foreign key: A field within a database table that is a primary key in another table (Refer to primary key)

Functional programming language: A programming language modeled around functions

Google DataFlow: A serverless data-processing service that reads data from the source, transforms it, and writes it in the destination location

Interpreted programming language: A programming language that uses an interpreter, typically another program, to read and execute coded instructions

Logical data modeling: Representing different tables in the physical data model

Object-oriented programming language: A programming language modeled around data objects

OLAP (Online Analytical Processing) system: A tool that has been optimized for analysis in addition to processing and can analyze data from multiple databases

OLTP (Online Transaction Processing) database: A type of database that has been optimized for data processing instead of analysis

Primary key: An identifier in a database that references a column or a group of columns in which each row uniquely identifies each record in the table (Refer to foreign key)

Python: A general purpose programming language

Response time: The time it takes for a database to complete a user request

Row-based database: A database that is organized by rows

Separated storage and computing systems: Databases where data is stored remotely, and relevant data is stored locally for analysis

Single-homed database: Database where all of the data is stored in the same physical location

Snowflake schema: An extension of a star schema with additional dimensions and, often, subdimensions

Star schema: A schema consisting of one fact table that references any number of dimension tables

Target table: The predetermined location where pipeline data is sent in order to be acted on

Terms and definitions from previous modules
A
Application programming interface (API): A set of functions and procedures that integrate computer programs, forming a connection that enables them to communicate 

Applications software developer: A person who designs computer or mobile applications, generally for consumers

B
Business intelligence (BI): Automating processes and information channels in order to transform relevant data into actionable insights that are easily available to decision-makers

Business intelligence governance: A process for defining and implementing business intelligence systems and frameworks within an organization

Business intelligence monitoring: Building and using hardware and software tools to easily and rapidly analyze data and enable stakeholders to make impactful business decisions

Business intelligence stages: The sequence of stages that determine both BI business value and organizational data maturity, which are capture, analyze, and monitor

Business intelligence strategy: The management of the people, processes, and tools used in the business intelligence process

D
Data analysts: People who collect, transform, and organize data

Data availability: The degree or extent to which timely and relevant information is readily accessible and able to be put to use

Data governance professionals: People who are responsible for the formal management of an organization’s data assets

Data integrity: The accuracy, completeness, consistency, and trustworthiness of data throughout its life cycle

Data maturity: The extent to which an organization is able to effectively use its data in order to extract actionable insights

Data model: A tool for organizing data elements and how they relate to one another

Data pipeline: A series of processes that transports data from different sources to their final destination for storage and analysis

Data visibility: The degree or extent to which information can be identified, monitored, and integrated from disparate internal and external sources

Data warehousing specialists: People who develop processes and procedures to effectively store and organize data

Deliverable: Any product, service, or result that must be achieved in order to complete a project

Developer: A person who uses programming languages to create, execute, test, and troubleshoot software applications

E
ETL (extract, transform, and load): A type of data pipeline that enables data to be gathered from source systems, converted into a useful format, and brought into a data warehouse or other unified destination system

Experiential learning: Understanding through doing

I
Information technology professionals: People who test, install, repair, upgrade, and maintain hardware and software solutions

Iteration: Repeating a procedure over and over again in order to keep getting closer to the desired result

K
Key performance indicator (KPI): A quantifiable value, closely linked to business strategy, which is used to track progress toward a goal

M
Metric: A single, quantifiable data point that is used to evaluate performance

P
Portfolio: A collection of materials that can be shared with potential employers

Project manager: A person who handles a project’s day-to-day steps, scope, schedule, budget, and resources

Project sponsor: A person who has overall accountability for a project and establishes the criteria for its success

S
Strategy: A plan for achieving a goal or arriving at a desired future state

Systems analyst: A person who identifies ways to design, implement, and advance information systems in order to ensure that they help make it possible to achieve business goals

Systems software developer: A person who develops applications and programs for the backend processing systems used in organizations

T
Tactic: A method used to enable an accomplishment

Transferable skill: A capability or proficiency that can be applied from one job to another

V
Vanity metric: Data points that are intended to impress others, but are not indicative of actual performance and, therefore, cannot reveal any meaningful business insights


Which type of tool can analyze data from multiple databases and is optimized for both analysis and processing? its OLAP

In a dimensional model, what might facts represent? its Attributes and Supporting details

In which stage of the ETL process is data delivered to a target system? is Loading

Fill in the blank: In a dimensional model, a foreign key is used to connect a _____ table to the appropriate fact table. is Dimension


In a dimensional model, what might facts represent? Select all that apply.
- Metrics
- Events


What are the key benefits of star and snowflake schemas? Select all that apply.
- High-scale information delivery
= More efficient output


Members of a business intelligence team are transitioning their current database schemas to a new, desired state. They add columns, remove elements, and make other improvements. What does this situation describe?
c. Database migration


----------------------------------------------------------------------------
Module 2: Dynamic database design

Data marts, data lakes, and the ETL process:

Data Marts and Data Lakes

- Data marts are smaller, focused databases tailored to specific business areas, offering a convenient way to access relevant data for particular BI projects.
- Data lakes, unlike structured data warehouses, store vast amounts of raw data in its native format until needed, allowing for flexibility and diverse data types.

ELT Pipelines

- ELT (Extract, Load, Transform) is a data integration process where data is first loaded into a destination system (like a data lake) and then transformed into a usable format.
- This approach allows BI professionals to handle diverse data types efficiently, optimize storage costs, and scale resources as needed.

ELT is a pipeline that extracts, loads, then transforms the data. It enables data to be gathered from data lakes, loaded into a unified destination system, and transformed into a useful format.



ETL vs ELT:

ELT:
- Data is extracted, loaded into the target system, and transformed as needed for analysis

- Data is transformed in the destination system, so no staging area is required

- ELT is a newer technology with fewer support tools built-in to existing technology

- ELT systems load all of the data, allowing users to choose which data to analyze at any time

- Calculations are added directly to the existing dataset

- ELT systems can ingest unstructured data from sources like data lakes

- Data has to be uploaded before data can be anonymized, making it more vulnerable

- ELT is well-suited to systems using large amounts of both structured and unstructured data

- Data loading is very fast in ELT systems because data can be ingested without waiting for transformations to occur, but analysis is slower

Data storage systems
Because ETL and ELT systems deal with data in slightly different ways, they are optimized to work with different data storage systems. Specifically, you might encounter data warehouses and data lakes. As a refresher, a data warehouse is a type of database that consolidates data from multiple source systems for data consistency, accuracy, and efficient access. And a data lake is a database system that stores large amounts of raw data in its original format until it’s needed. While these two systems perform the same basic function, there are some key differences:

Data Warehouse:
- Data has already been processed and stored in a relational system
- The data’s purpose has already been assigned, and the data is currently in use
- Making changes to the system can be complicated and require a lot of work

Data Lake:
- Data is raw and unprocessed until it is needed for analysis; additionally, it can have a copy of the entire OLTP or relational database
- The data’s purpose has not been determined yet
- Systems are highly accessible and easy to update



Currently, ETL systems that extract, transform and load data, and ELT systems that extract, load, and then transform data are common ways that pipeline systems are constructed to move data where it needs to go. Understanding the differences between these systems can help you recognize when you might want to implement one or the other. And, as business and technology change, there will be a lot of opportunities to engineer new solutions using these data systems to solve business problems.


The five factors of database performance:

Understanding Database Workload

- Workload represents the combined total of transactions, queries, analyses, and system commands that a database system handles at any given time.
- It's important to note that a database's workload can vary significantly due to factors like job processing and the number of active users.


The Role of Throughput

- Throughput measures the overall capacity of a database's hardware and software to process requests effectively.
- Factors like input/output speed, CPU speed, parallel processing capabilities, and the efficiency of the database management system all contribute to throughput.

Importance of Resources and Optimization

- Resources encompass the hardware and software components within a database system, including crucial elements like disk space and memory.
- Optimization focuses on maximizing the speed and efficiency of data retrieval to maintain high levels of database performance.

The factor of contention should be addressed. Contention occurs when two or more components attempt to use a single resource in a conflicting way.


The five factors Database:
- Workload
The combination of transactions, queries, data warehousing analysis, and system commands being processed by the database system at any given time.

- Throughput
The overall capability of the database’s hardware and software to process requests.

- Resources
The hardware and software tools available for use in a database system.

- Optimization
Maximizing the speed and efficiency with which data is retrieved in order to ensure high levels of database performance.

- Contention
When two or more components attempt to use a single resource in a conflicting way.


database optimization, a crucial aspect of ensuring good database performance. You're learning about common reasons for slow databases, such as inefficient queries, lack of indexing, and fragmented data. The lecture emphasizes that by addressing these issues, BI professionals can significantly improve data retrieval speed and overall database efficiency. Remember, database optimization is an ongoing process, requiring continuous monitoring and adjustments to maintain optimal performance.


What is the process of dividing a database into distinct, logical parts in order to improve query processing and increase manageability?

Data partitioning is the process of dividing a database into distinct, logical parts in order to improve query processing and increase manageability. Ensuring data is partitioned appropriately is a key part of database performance optimization.



Indexes, partitions, and other ways to optimize

Next, you will learn more about different ways you can optimize your database to read data, including indexing and partitioning, queries, and caching.

Indexes
Sometimes, when you are reading a book with a lot of information, it will include an index at the back of the book where that information is organized by topic with page numbers listed for each reference. This saves you time if you know what you want to find– instead of flipping through the entire book, you can go straight to the index, which will direct you to the information you need.

Indexes in databases are basically the same– they use the keys from the database tables to very quickly search through specific locations in the database instead of the entire thing. This is why they’re so important for database optimization– when users run a search in a fully indexed database, it can return the information so much faster.  For example, a table with columns ID, Name, and Department could use an index with the corresponding names and IDs.

Partitions
Data partitioning is another way to speed up database retrieval. There are two types of partitioning: vertical and horizontal. Horizontal partitioning is the most common, and involves designing the database so that rows are organized by logical groupings instead of stored in columns. The different rows are stored in different tables– this reduces the index size and makes it easier to write and retrieve data from the database.

Instead of creating an index table to help the database search through the data faster, partitions split larger, unwieldy tables into much more manageable, smaller tables. 


Queries
Queries are requests for data or information from a database. In many cases, you might have a collection of queries that you run regularly; these might be automated queries that generate reports, or regular searches made by users.


Caching
Finally, the cache can be a useful way to optimize your database for readability. Essentially, the cache is a layer of short-term memory where tables and queries can be stored. By querying the cache instead of the database system itself, you can actually save on resources. You can just take what you need from the memory.



Partitions and indexes help optimize a database by creating shortcuts to specific rows and dividing large datasets into smaller, more manageable tables. In addition, partitions and indexes enable faster and more efficient databases, which makes it easier to pull data for analysis or visualization.



Activity Exemplar: Partition data and create indexes in BigQuery:

This activity demonstrates the impact of using partitions and indexes (known as clusters in BigQuery) in database tables. You can use them to optimize query performance and minimize processing costs. In this exercise, applying partitions and clustering means that BigQuery can break all 41,025 records into smaller, more manageable tables to read. The benefits of partitioning will be even more evident with larger datasets. Use this technique to optimize database performance in your future projects.




Case study: Deloitte - Optimizing outdated database systems

Company background
Deloitte collaborates with independent firms to provide audit and assurance, consulting, risk and financial advisory, risk management, tax, and related services to select clients. Deloitte’s brand vision is to be a standard of excellence within the field and to uphold their brand values as they develop leading strategies and cutting edge tools for clients to facilitate their business. These values include integrity, providing outstanding value to clients, commitment to community, and strength from cultural diversity. 

The challenge
Because of the size of the company and their ever-evolving data needs, the database grew and changed to match current problems without time to consider long-term performance. Because of this, the database eventually grew into a collection of unmanaged tables without clear joins or consistent formatting. This made it difficult to query the data and transform it into information that could effectively guide decision making. 

The need for optimization appeared gradually as the team had to continually track data and had to repeatedly test and prove the validity of the data.  With a newly optimized database, the data could more easily be understood, trusted, and used to make effective business decisions.

Primarily, this database contained marketing and financial data that would ideally be used to connect marketing campaigns and sales leads to evaluate which campaigns were successful. But because of the current state of the database, there was no clear way to tie successes back to specific marketing campaigns and evaluate their financial performance. The biggest challenge to this initiative was programming the external data sources to feed data directly into the new database, rather than into the previous tables that were scheduled to be deprecated.  Additionally, the database design needed to account for tables that represented the lifecycle of the data and designed with joins that could easily and logically support different data inquiries and requests.


In order to consider the database optimization project a success, the BI team wanted to address the following issues:

Were the necessary tables and columns consolidated in a more useful way?

Did the new schema and keys address the needs of analyst teams?

Which tables were being queried repeatedly and were they accessible and logical?

What sample queries could promote confidence in the system for users?


The results
Deloitte’s BI team recognized that, while the database had been continually updated to address evolving business needs, it had grown harder to manage over time. In order to promote greater database performance and ensure their database could meet their needs, the BI team collaborated with database engineers and administrators to design a custom database architecture that thoughtfully addressed the business needs of the organization. For example, the new database structure helped build connections between tables tracking marketing campaigns over time and their successes, including revenue data and regional locations.

This database optimization effort had a lot of benefits. The greatest benefit was the organization’s ability to trust their data–the analyst team didn’t have to spend as much time validating the data before use because the tables were now organized and joined in more logical ways. The new architecture also promoted simpler queries. Instead of having to write hundreds of lines of complicated code to return simple answers, the new database was optimized for simpler, shorter queries that took less time to run.

This provided benefits for teams across the organization:

The marketing team was able to get better feedback on the value created by specific campaigns.

The sales team could access specific information about their regions and territories, giving them insights about possible weaknesses and opportunities for expansion.

The strategy team was able to bridge the gap between the marketing and sales teams, and ultimately create actionable OKRs (Objectives and Key Results) for the future.


The databases where your organization stores their data are a key part of the BI processes and tools you create–if the database isn’t performing well, it will affect your entire organization and make it more difficult to provide stakeholders with the data they need to make intelligent business decisions. Optimizing your database promotes high performance and a better user experience for everyone on your team.


Optimizing queries will make your pipeline operations faster and more efficient. In your role as a BI professional, you might work on projects with extremely large datasets. For these projects, it’s important to write SQL queries that are as fast and efficient as possible. Otherwise, your data pipelines might be slow and difficult to work with.

A data mart is a subject-oriented database that can be a subset of a larger data warehouse. This means it is a convenient way to access the data pertaining to specific areas or departments of a business. 

They consider throughput. Throughput describes the overall capability of the database’s hardware and software to process requests.

Fragmentation most often occurs when the data is used frequently, when new data files are created, or when existing data files are modified or deleted.




----------------------------------------------------------------------------
Module 3: Optimize ETL processes

The importance of quality testing

Quality testing is the process of checking data for defects in order to prevent system failures.

Why Quality Testing Matters

- Essential for preventing system failures and ensuring a smooth workflow in BI.
- Involves checking data for defects to maintain the pipeline's proper functioning.



Seven Key Elements of Quality Testing

- Completeness: Verifying that no data components or measures are missing. For example, ensuring a sales data pipeline includes all weeks and key metrics.
- Consistency: Confirming data compatibility and agreement across all systems, such as an HR database and a payroll system.
- Conformity: Ensuring data fits the required format of the destination system. For instance, date formats in sales data should match the destination table.
- Accuracy: Checking if data reflects real and accurate values, addressing any errors or typos from the source.
- Redundancy: Avoiding unnecessary duplication of data, which consumes resources during transfer and storage.
- Integrity: Ensuring the accuracy, completeness, consistency, and trustworthiness of data, including checking for missing relationships between data values.
- Timeliness: Confirming data is current and updated, allowing stakeholders to gain the most relevant insights.


When quality testing, a business intelligence professional confirms data conformity in order to ensure the data fits the required destination format.



Seven elements of quality testing:

Checking for data quality involves ensuring the data is trustworthy before reaching its destination. When considering what checks you need to ensure the quality of your data as it moves through the pipeline, there are seven elements you should consider:


Completeness: Does the data contain all of the desired components or measures?

Consistency: Is the data compatible and in agreement across all systems?

Conformity: Does the data fit the required destination format?

Accuracy: Does the data conform to the actual entity being measured or described?

Redundancy: Is only the necessary data being moved, transformed, and stored for use?

Timeliness: Is the data current?

Integrity: Is the data accurate, complete, consistent, and trustworthy? (Integrity is influenced by the previously mentioned qualities.) Quantitative validations, including checking for duplicates, the number of records, and the amounts listed, help ensure data’s integrity.


Common issues
There are also some common issues you can protect against within your system to ensure the incoming data doesn’t cause errors or other large-scale problems in your database system:

Check data mapping: Does the data from the source match the data in the target database?

Check for inconsistencies: Are there inconsistencies between the source system and the target system?

Check for inaccurate data: Is the data correct and does it reflect the actual entity being measured?

Check for duplicate data: Does this data already exist within the target system?



Checking for data quality involves ensuring the data is trustworthy before reaching its destination.


Quantitative validations help ensure data’s integrity


Completeness is a quality testing step that involves confirming that the data contains all desired measures or components.



Conformity from source to destination:

Schema Validation

- Schema validation is a process that ensures the source system's data schema matches the target database's schema, preventing system failures.
- Database tools offer various schema validation options to check incoming data against the destination schema requirements, such as ensuring a column contains only numerical data or that an ID number is unique.

Data Dictionaries and Data Lineages

- Data dictionaries define the content, format, structure, and relationships of data objects within a database, ensuring consistency and alignment across teams.
- Data lineage tracks the origin, movement, and transformation of data, allowing you to identify and address errors by understanding the data's journey through the system.




Data dictionaries
A data dictionary is a collection of information that describes the content, format, and structure of data objects within a database, as well as their relationships. This can also be referred to as a metadata repository because data dictionaries use metadata to define the use and origin of other pieces of data.


Data lineages
A data lineage describes the process of identifying the origin of data, where it has moved throughout the system, and how it has transformed over time. This can be really helpful for BI professionals, because when they do encounter an error, they can actually track it to the source using the lineage. Then, they can implement checks to prevent the same issue from occuring again.

For example, imagine your system flagged an error with some incoming data about the number of sales for a particular item. It can be hard to find where this error occurred if you don’t know the lineage of that particular piece of data– but by following that data’s path through your system, you can figure out where to build a check.



Tools such as data dictionaries and data lineages are useful for preventing inconsistencies as data is moved from source systems to its final destination. It is important that users accessing and using that data can be confident that it is correct and consistent. These tools actively track and validate data throughout its journey, ensuring it's reliable for analysis and decision-making. This is key for building trustworthy reports and dashboards as a BI professional!



Check your schema:
the importance of schema governance in maintaining data consistency and preventing errors during data ingestion through a case study of an educational non-profit.

Schema Governance in Action

- The non-profit needs to combine data from multiple school databases to measure educational outcomes, making schema governance crucial for consistency.
- They already have a data dictionary and data lineage in place, which are essential tools for schema governance.

Data Dictionary and Validation

- The data dictionary defines the format and structure of each data column, ensuring that incoming data conforms to the established standards.
- If incoming data doesn't match the data dictionary definition, the schema validation process flags the error before it enters the database.

Data Lineage and Error Tracing

- Data lineage tracks the origin, movement, and transformations of data throughout the system.
- When a schema validation error occurs, data lineage helps pinpoint the source of the error and identify areas for improvement in the data pipeline.



Schema-validation checklist:

Schema validation is a process used to ensure that the source system data schema matches the target database data schema. This is important because if the schemas don’t align, it can cause system failures that are hard to fix. Building schema validation into your workflow is important to prevent these issues.

Common issues for schema validation
The keys are still valid: Primary and foreign keys build relationships between tables in relational databases. These keys should continue to function after you have moved data from one system into another.

The table relationships have been preserved: The keys help preserve the relationships used to connect the tables so that keys can still be used to connect tables. It’s important to make sure that these relationships are preserved or that they are transformed to match the target schema.

The conventions are consistent: The conventions for incoming data must be consistent with the target database’s schema. Data from outside sources might use different conventions for naming columns in tables– it’s important to align these before they’re added to the target system.

Using data dictionaries and lineages
You’ve already learned quite a bit about data dictionaries and lineages. As a refresher, a data dictionary is a collection of information that describes the content, format, and structure of data objects within a database, as well as their relationships. And a data lineage is the process of identifying the origin of data, where it has moved throughout the system, and how it has transformed over time. These tools are useful because they can help you identify what standards incoming data should adhere to and track down any errors to the source.


Schema validation is a useful check for ensuring that the data moving from source systems to your target database is consistent and won’t cause any errors. Building in checks to make sure that the keys are still valid, the table relationships have been preserved, and the conventions are consistent before data is delivered will save you time and energy trying to fix these errors later on.



What are Business Rules?

Business rules are specific restrictions placed on a database to reflect how an organization uses its data, ensuring data integrity and consistency.

For example, a library database might have a rule limiting patrons to five checked-out books, preventing errors and aligning with library policies.

Importance and Verification of Business Rules:
- Business rules heavily influence database design, impacting data collection, relationships, information output, and security.
- Verifying business rules involves checking if incoming data adheres to these rules before entering the database, similar to schema validation but focused on business logic.


A business rule is a statement that creates a restriction on specific parts of a database. It helps prevent errors within the system.



Business Rules:
For instance, if a company values cross-functional collaboration, there may be rules about at least 2 representatives from two teams checking off completion on some data set. They affect what data is collected and stored, how relationships are defined, what kind of information the database provides, and the security of the data. In this reading, you will learn more about the development of business rules and see an example of business rules being implemented in a database system.


Imposing business rules
Business rules are highly dependent on the organization and their data needs. This means business rules are different for every organization. This is one of the reasons why verifying business rules is so important; these checks help ensure that the database is actually doing the job you need it to do. But before you can verify business rules, you have to implement them.


Business rules determine what data is collected and stored, how relationships are defined, what kind of information the database provides, and the security of the data. These rules heavily influence how a database is designed and how it functions after it has been set up. Understanding business rules and why they are important is useful as a BI professional because this can help you understand how existing database systems are functioning, design new systems according to business needs, and maintain them to be useful in the future.


Database performance testing in an ETL context:


How database performance affects your pipeline
Database performance is the rate that a database system is able to provide information to users. Optimizing how quickly the database can perform tasks for users helps your team get what they need from the system and draw insights from the data that much faster.

Your database systems are a key part of your ETL pipeline– these include where the data in your pipeline comes from and where it goes. The ETL or pipeline is a user itself, making requests of the database that it has to fulfill while managing the load of other users and transactions. So database performance is not just key to making sure the database itself can manage your organization’s needs– it’s also important for the automated BI tools you set up to interact with the database


Key factors in performance testing
Earlier, you learned about some database performance considerations you can check for when a database starts slowing down. Here is a quick checklist of those considerations:

Queries need to be optimized

The database needs to be fully indexed

Data should be defragmented

There must be enough CPU and memory for the system to process requests




As a BI professional, you need to know that your database can meet your organization’s needs. Performance testing is a key part of the process. Not only is performance testing useful during database building itself, but it’s also important for ensuring that your pipelines are working properly as well. Remembering to include performance testing as a way to check your pipelines will help you maintain the automated processes that make data accessible to users!


Pipeline layers
Pipelines can have many different stages of processing. These stages, or layers, help ensure that the data is collected, aggregated, transformed, and staged in the most effective and efficient way. For example, it’s important to make sure you have all the data you need in one place before you start cleaning it to ensure that you don’t miss anything. There are usually four layers to this process: staging, harmonization, validation, and reconciliation. After these four layers, the data is brought into its target database and an error handling report summarizes each step of the process.


1. Staging layer
First, the original data is brought from the source systems and stored in the staging layer. In this layer, Arsha ran the following defensive checks:

Compared the number of records received and stored

Compared rows to identify if extra records were created or records were lost

Checked important fields, such as amounts, dates, and IDs


2. Harmonization layer
The harmonization layer is where data normalization routines and record enrichment are performed. This ensures that data formatting is consistent across all the sources. To harmonize the data, Arsha ran the following defensive checks:

Standardized the date format

Standardized the currency

Standardized uppercase and lowercase stylization

Formatted IDs with leading zeros

Split date values to store the year, month, and day in separate columns

Applied conversion and priority rules from the source systems

When a record couldn’t be harmonized, she moved it to Error Handling. She marked all of the records that moved to the next layer as “processed.”


3. Validations layer
The validations layer is where business rules are validated. As a reminder, a business rule is a statement that creates a restriction on specific parts of a database. These rules are developed according to the way an organization uses data. Arsha ran the following defensive checks:

Ensured that values in the “department” column were not null, since “department” is a crucial dimension

Ensured that values in the “service type” column were within the authorized values to be processed

Ensured that each billing record corresponded to a valid processed contract

Again, when a record couldn’t be validated, she moved it to error handling. She marked all the records that moved to the next layer as “processed.”


4. Reconciliation layer
The reconciliation layer is where duplicate or illegitimate records are found. Here, Arsha ran defensive checks to find the following types of records:

Slow-changing dimensions

Historic records

Aggregations

As with the previous layers, Arsha moved the records that didn't pass the reconciliation rules to Error Handling. After this round of defensive checks, she brought the processed records into the BI and Analytics database (OLAP)



Error handling reporting and analysis
After completing the pipeline and running the defensive checks, Arsha made an error handling report to summarize the process. The report listed the number of records from the source systems, as well as how many records were marked as errors or ignored in each layer. The end of the report listed the final number of processed records.


Case study: FeatureBase, Part 2: Alternative solutions to pipeline systems:

This case study with FeatureBase will focus on the Analyze stage of the BI process, where you examine relationships in the data, draw conclusions, make predictions, and drive informed decision-making. This follows an earlier case study, where you explored the Capture phase of FeatureBase’s project. In a follow-up case study, you’ll learn about how FeatureBase addressed the Monitor stages of this project to solve their business problem. Like the previous FeatureBase case study, you’ll consider the problem, process, and solution for this stage of the project.

Their core technology, FeatureBase, is the first OLAP database built entirely on bitmaps that power real-time analytics and machine learning applications by simultaneously executing low latency, high throughput, and highly concurrent workloads. Last time, you learned about a business problem the FeatureBase team was facing: they realized that customers were falling off during the sales cycle, but that their data collection didn’t have the necessary measurements to investigate when and why this was happening. The first step to addressing this issue was collaborating across sales, marketing, and leadership teams to determine what data they needed to understand when customers were falling off. Then, they could use that insight to investigate and address those issues for future sales. In this reading, you’re going to focus on the database tools FeatureBase uses to collect data for monitoring and reporting. 



Feature-oriented databases:
Feature-oriented databases provide an alternate approach to data prep by automating the feature extraction as the first step. The feature-oriented approach enables real-time analytics and AI initiatives because the data or "features" are already in a model-ready data format that is instantly accessible and reusable across the organization, without the need to re-copy or re-pre-process.


Fine-tuning features:
At that point in this project’s cycle, the team didn’t have metrics built into their system to find out when customers weren’t completing the sales process, which was key to investigating why customers were dropping off. Having determined what their system was lacking, the team encoded these new features into the collection process– they recreated their original sales funnel with new attributes about customers at every stage of the sales cycle. These new features were fed into their database model, which began training to identify patterns, allowing them to immediately draw insights from their pool of data.  

One of the attributes they added to the data collection process tracked exactly when customers were dropping off.  They discovered that most customers who didn’t complete the sale dropped off during the technical validation stage. This is the point at which the FeatureBase team would set up FeatureBase within the customer’s system so they can try the product for themselves. The team realized that this was the critical stage they needed to investigate more. They theorized that customers weren’t leaving the technical validation stage confident about their ability to adopt this new technology. They also wondered if customers were having concerns about the reliability and stability of the product since FeatureBase would replace their current, still-functioning database system. To understand this better, they would need to explore the customer data gathered at this stage in their dashboard.

The next step
As a BI professional, you might find yourself working with a variety of database technologies connected with pipeline systems like more traditional row based technologies and newer alternatives such as FeatureBase. Understanding your tools and how they operate will help you focus on what’s most important–empowering your team with access to the answers they need. As they continued investigating their problem, the FeatureBase team found that most customers who did not complete the sales process were falling off in the technical validation stage. 

This is the point at which FeatureBase was being implemented in the customer’s data environment to determine if it was actually functional for them. This is how the FeatureBase team can showcase FeatureBase’s utility and provide proof that it is a workable solution for a customer’s needs. 

This scenario describes establishing business rules. A business rule is a statement that creates a restriction on specific parts of a database. It helps determine if a database is performing as intended. 


During the data-transfer process, incoming data should be compared to business rules before loading it into the database.



A business intelligence professional wants to avoid system failures. They check over their data in order to identify missing data, inconsistent data, or any other data defects. What does this scenario describe?
d. Quality testing

A business intelligence professional is confirming that their data contains all desired components or measures. Which quality testing validation element does this involve?
a. Completeness

A business intelligence professional is working with a data warehouse. They perform various tasks to confirm that the data is timely and the pipeline is ingesting the latest information. For what reasons is this an important element of business intelligence? Select all that apply.
- To ensure the data is updated properly
- To provide relevant insights
- To have the most current information

Fill in the blank: To ensure _____ from source to destination, business intelligence professionals use schema validation, data dictionaries, and data lineages.
c. Conformity


What are the goals of schema validation? Select all that apply. 
- To preserve table relationships
- To confirm the validity of database keys 
- To ensure consistent conventions

Which of the following statements accurately describe data dictionaries and data lineages? Select all that apply. 

- A data dictionary is a collection of information that describes the content, format, and structure of data objects within a database. 
- A data lineage describes the process of identifying the origin of data, where it has moved throughout the system, and how it has transformed over time. 


A business intelligence professional establishes what data will be collected, stored, and provided in a database. They also confirm how relationships are defined and the security of the data. What process does this scenario describe?
c. Creating business ruless


----------------------------------------------------------------------------
Module 4: Course 2 end-of-course project

Explore Course 2 end-of-course project scenarios:

When you approach a project using structured thinking, you will often find that there are specific steps you need to complete in a specific order. The end-of-course projects in the Google Business Intelligence certificate were designed with this in mind. The challenges presented in each course represent a single milestone within an entire project, based on the skills and concepts learned in that course. 

Background: 

In this fictitious workplace scenario, the imaginary company Cyclistic has partnered with the city of New York to provide shared bikes. Currently, there are bike stations located throughout Manhattan and neighboring boroughs. Customers are able to rent bikes for easy travel among stations at these locations. 

Scenario:

You are a newly hired BI professional at Cyclistic. The company’s Customer Growth Team is creating a business plan for next year. They want to understand how their customers are using their bikes; their top priority is identifying customer demand at different station locations. Previously, you gathered information from your meeting notes and completed important project planning documents. Now you are ready for the next part of your project! 

Course 2 challenge:

Use project planning documents to identify key metrics and dashboard requirements

Observe stakeholders in action to better understand how they use data

Gather and combine necessary data

Design reporting tables that can be uploaded to Tableau to create the final dashboard

Note: The story, as well as all names, characters, and incidents portrayed, are fictitious. No identification with actual people (living or deceased) is intended or should be inferred. The data shared in this project has been created for pedagogical purposes.


Google Fiber:

Background: 

Google Fiber provides people and businesses with fiber optic internet. Currently, the customer service team working in their call centers answers calls from customers in their established service areas. In this fictional scenario, the team is interested in exploring trends in repeat calls to reduce the number of times customers have to call in order for an issue to be resolved. 

Scenario:

You are currently interviewing for a BI position on the Google Fiber call center team. As part of the interview process, they ask you to develop a dashboard tool that allows them to explore trends in repeat calls. The team needs to understand how often customers call customer support after their first inquiry. This will help leadership understand how effectively the team can answer customer questions the first time. Previously, you gathered information from your meeting notes and completed important project planning documents. Now you’re ready for the next part of your project! 

Course 2 challenge:

Use project planning documents to identify key metrics and dashboard requirements

Consider best tools to execute your project

Gather and combine necessary data

Design reporting tables that can be uploaded to Tableau to create the final dashboard

Key Takeaways
In Course 2, The Path to Insights: Data Models and Pipelines, you focused on understanding how data is stored, transformed, and delivered in a BI environment. 

Course 2 skills:
Combine and transform data

Identify key metrics

Create target tables

Practice working with BI tools




Course 2 workplace scenario overview: Cyclistic:

Cyclistic datasets: (ETL Querying)


Observe the Cyclistic team in action
Understanding your stakeholders and how they use the data is key to developing business intelligence solutions that will address their specific needs. In addition to meeting with stakeholders to discuss their project requirements, it can also be useful to observe the team at work and identify any patterns or frequently asked questions. 

In this reading, you’ll discover some of the ways that the Cyclistic team uses the data. This can inform the BI solution you develop for them and help you design the final reporting dashboard to be specific to their needs and useful to the whole team. As a BI professional, recognizing a team’s needs and customizing their systems can support their work as they make key business decisions. 

The team at work
As you learned during your previous meeting with Cyclistic, the product development team has begun planning for the next year of Cyclistic’s bike-sharing program. Cyclistic's Customer Growth Team is creating a business plan for next year. The team wants to understand how their customers are using their bikes; their top priority is identifying customer demand at different station locations. The Cyclistic team posed an important primary question:

How can we apply customer usage insights to inform new station growth?

Answering these questions starts with the data from the Cyclistic bikes themselves, which the team has provided you, and the reporting dashboard the team uses to gain insights. In addition to the explicit requests the stakeholders made, you realize a few key things about the team's current processes. 

First, you realize that there are stakeholders from a variety of different departments accessing and using this data with different levels of technical expertise. There are stakeholders from these teams:

Product development

Customer data

Engineering

Data analytics

Data warehousing

API

IT

Cyclistic executive

Project management

For example, you realize that Earnest Cox, the VP of product development, is often requesting high-level insights into the data and rarely needs detailed overviews of the data. Alternatively, Tessa Blackwell from the data analytics team does explore the data in-depth and spends a lot more time reviewing the dashboard views. As you develop your reporting tools, you will want to find a way to answer both of these stakeholders’ needs. 

Additionally, one of your coworkers finds out you’re working on this project and shares a dataset they created recently for a project of their own that they think might help you: 
NYC zip codes
. This dataset provides the zip codes for the different neighborhoods and boroughs in New York City; this will let you compare the bike data to the weather data more easily since you will be able to match the locations more accurately. It will also help you develop your map visualization later on.  

Key takeaways
As you prepare to create the pipeline system that will deliver data to your reporting tables and eventually your dashboard, recognizing the different kinds of users, their specific needs and questions, and how they are currently using the data can help guide your development process. In this case, observing the Cyclistic team in action revealed that there are users who need different levels of detail and have different technical abilities. Additionally, you gained a useful tool from a colleague that will help you explore multiple datasets for this project. You can use these discoveries to design a BI solution that really addresses this organization’s unique needs–and demonstrate your skill and flexibility to future employers!


In this activity, you will use your knowledge of SQL and potentially Google Dataflow to combine and move the key datasets you identified for the Cyclistic project into a target table. This represents the extraction phase of an ETL pipeline, when data is pulled from different sources and moved to its destination. You will use the table you create in this activity to develop the final dashboard for stakeholders. As you complete this activity, remember to refer to the previous work you did when
  completing the business intelligence project documents for Cyclistic 
 for details about the Cyclistic project, as well as the activity to
  create a target table in BigQuery 
 for a refresher on target tables. 

Review the following scenario. Then, complete the activity. As a reminder, the end-of-course project activities are more open to your personal interpretation than other activities in this program. This is to give you an opportunity to practice the skills you have been learning in your own way. If you need help or feel stuck, you can always discuss your work with other learners in the discussion forums or review the exemplar to help guide your process.

The product development team at Cyclistic has begun developing their business plan for next year. In order to build a better Cyclistic, the team needs to understand how customers are currently using the bikes, how location and other factors impact demand, and what stations get the most traffic. The Cyclistic team has a few goals:

Understand current customers needs, what makes a successful product, and how new stations might alleviate demand in different geographical areas

Understand current usage of bikes at different locations 

Apply customer usage insights to inform new station growth

Understand how different users (subscribers and non-subscribers) use the bikes




Course 2 workplace scenario overview: Google Fiber

Google Fiber datasets (Dashboard)



[Optional] Merge Google Fiber datasets in Tableau:


Review the following scenario. Then, complete the activity. As a reminder, the end-of-course project activities are more open to your personal interpretation than other activities in this program. This is to give you an opportunity to practice the skills you have been learning in your own way. If you need help or feel stuck, you can always discuss your work with other learners in the discussion forums or review the exemplar to help guide your process.

The Google Fiber customer service team’s goal is to understand how often customers are calling customer support after their first inquiry; this will help leadership understand how effectively the team is able to answer customer questions the first time. The dashboard you create should demonstrate an understanding of this goal and provide your stakeholders with insights about repeat caller volumes in different markets and the types of problems they represent. As part of the interview process, they have asked you to create a dashboard that will: 

Help them understand how often customers are calling customer support after their first inquiry; this will help leadership understand how effectively the team is able to answer customer questions the first time

Provide insights into the types of customer issues that seem to generate more repeat calls

Explore repeat caller trends in the three different market cities

Design charts so that stakeholders can view trends by week, month, quarter, and year. 

You met with stakeholders to complete project planning documents and uploaded the necessary tables into your BigQuery project space. 



From 2 projects above we must confirm that:

1. The relevant datasets have been accessed and downloaded.
2. The relevant datasets are uploaded to the BigQuery project space.
3. The relevant datasets are applied to the project. 
4. The source tables are merged using SQL code.
5. The metrics to be shared in the final dashboard are included.
6. The metrics to be included in the final dashboard are in the target table.
7. Data is clean to ensure consistency across multiple datasets.
8. Data is transformed to ensure consistency across multiple datasets.
9. A final reporting table is generated. 
10. The reporting table includes all required components in order to prepare the dashboard for creation.








============================================================================
3. Dashboards and Reports



----------------------------------------------------------------------------
Module 1: Business intelligence visualizations

A business intelligence professional might choose to use a dynamic visualization if they want it to be interactive or change over time.

Types of Dashboards
Often, BI professionals will tailor a dashboard for a specific purpose. The three most common categories are:

1. Strategic: focuses on long-term goals and strategies at the highest level of metrics

A wide range of businesses use strategic dashboards when evaluating and aligning their strategic goals. These dashboards provide information over the longest time frame—from a single financial quarter to years. They typically contain information that is useful for enterprise-wide decision-making. For example, a strategic dashboard could focus on key performance indicators over a year.



2. Operational: tracks short-term performance and intermediate goals
Operational dashboards are arguably the most common type of dashboard. Because these dashboards contain information on a time scale of days, weeks, or months, they can provide performance insight almost in real-time. This enables businesses to track and maintain their immediate operational processes in light of their strategic goals. An operational dashboard could focus on customer service team performance.


3. Analytic: consists of the datasets and the mathematics used in these sets
Analytic dashboards contain the details involved in the use, analysis, and predictions made by data scientists. Data science teams usually create and maintain the most technical category, analytic dashboards. An example of an analytic dashboard could focus on metrics for a company’s financial performance.


A dynamic visualization is interactive or changes over time. This allows BI professionals to track and monitor the data relevant to answering ongoing business questions.

The primary goal of the stories told by dashboards is to provide important information about what’s currently happening.

Stakeholders use business intelligence tools in order to make informed decisions. 



In order to be effective, your mockups should have clear, bold details and be easy to understand. You don’t need to spend time defining color schemes, fonts, or other kinds of visual style. Your mockups just need to clearly communicate your intentions.



Activity Exemplar: Design a data visualization mockup:
To review the exemplar for this course item, click the following link.

Link to exemplar: 
Medical practice mockup sketch

OR

You can download the exemplar directly from the following attachment below.


Mockups are personal tools that you share with a stakeholder. They should have clear, bold details and be easy to understand. You don’t need to spend time defining color schemes, fonts, or other kinds of visual style. This should just be the first instance of translating your business needs into a visual representation. Your mockups don’t need to be perfect. They just need to communicate your intentions.

Mockups focus on the arrangement of objects and the contents of each object. An example of the level of detail it should include is “This big chart will go on the top of the main page. It will be a bar chart that represents the frequency of patient visits. Next to it will be this chart, which answers the question about the time between each visit.”

Mockups can include annotations, but they should feature minimal text overall. The majority of your details should go in your planning docs.

In this activity’s scenario, your contact at the private practice wants to answer the following questions:

What is the frequency of visits from returning patients?

What is the average length of time between visits per patient and/or diagnosis?

How many patients are being treated for a specific condition? How does their visit frequency differ from patients who only need general visits?

What are the results of the five-question surveys that patients are asked to complete after each visit?

In this mockup example, the following section of the dashboard includes interactive filters that stakeholders can use to view data at various timescales.


This company is experiencing a tool problem, which is a dashboard issue involving its hardware or software.




----------------------------------------------------------------------------
Module 2: Visualize results

Design trade-offs when building a dashboard;


how to balance different factors to achieve the best outcome for your stakeholders.

Understanding Trade-offs

Trade-offs in BI involve making choices to prioritize certain aspects of data representation over others.


For example, you might choose a yearly timescale for a chart to present a simpler view, sacrificing the detailed insights a quarterly timescale would provide.



Pre-aggregation: Speed vs. Flexibility

- Pre-aggregation can speed up dashboards by performing calculations within the database before sending data to the visualization tool.

- However, pre-aggregation can reduce data flexibility. If you pre-aggregate sales data by region, you lose the ability to easily analyze it by store size later on.

Pre-aggregation is the process of performing calculations on data while it is still inside a database.


Often, as a BI professional, you will encounter language that means different things in different contexts. By paying close attention, asking questions, and thinking critically, you can ensure that you and your team stay on the same page. In this case, the difference between project scope and dashboard scope is useful to understand as you communicate with stakeholders about their expectations with the dashboard specifically, and not the entire project.


Case study: Allegis Group - Visualizing key data to understand and advance employee performance:

Company background
Allegis Group is a talent solutions firm comprising several specialized companies that solve business problems for clients across a variety of industries. The firm does this by identifying great candidates for its client companies. Allegis Group recruiters identify talent, and then connect them with career opportunities at client organizations. Each time a candidate is placed, that's considered a sale for Allegis Group. 




Scenario
The talent recruitment process requires oversight and progress reports. Therefore, Allegis Group conducts its own data analytics and BI research to track the performance of its new recruiters. To challenge assumptions about how long it takes for new hires to meet sales goals, Allegis Group leaders wanted data insights into how each cohort of new recruiters performed within the first year. The goal was to create a dashboard that tracks the sales revenue made by the newly onboarded recruiters. With this tool, Allegis Group would be able to measure the new hires’ ability to bring quality talent to clients. Then, these insights would be communicated to internal company leaders in order to improve processes moving forward.

The business questions
Through internal brainstorming, Allegis Group leaders confirmed that they needed answers to the following questions:

How are new recruiters performing over time?

What are the best months of the year to hire new recruiters?

Are performance goals for new hires correctly aligned?

Are new recruiters now finding success more quickly than those hired a year ago?

To answer these questions, Allegis Group’s BI team collected data on how many months recruiters had been with the company. They also collected data on the sales revenue generated by recruiters within their first year. The recruiters were divided into cohorts depending on when they were brought onto Allegis Group’s roster. Then, each cohort could be measured by how much revenue they made based on how long they had been with Allegis Group. This would enable Allegis Group to review and evaluate the success of each cohort over the first year of employment. The dashboard would help decision-makers learn about retention rates, the ideal hiring months, and how to improve overall hiring practices.

The solution
With these questions in mind and the data gathered, the team began creating the dashboard. They first visualized the main metric, which was the sales (in dollars) per number of individuals in the initial recruiter cohort population. This metric represented the total sales for a month divided by the number of recruiters that were initially hired in that cohort. Then, they defined the data source and created a data view. Finally, they decided on a monthly schedule to refresh and update the data with the cohort’s monthly progress report.



The results
The resulting dashboard had several strengths:

The flexibility to answer each of the team’s questions in one place

A monthly refresh for continued value

Quick visual comparison within a cohort (rows) and between different cohorts at the same point in time (columns)

Interactivity for quick comparisons of cohort retention and performance

Ease of adding new metrics*


Design resource guide:
Use a visualization framework: Frameworks like 
the McCandless Method
 and 
Kaiser Fung’s Junk Charts Trifecta Checkup
 can help you organize your thoughts about data visualization and give you a useful checklist to reference.

Choose the right chart: Part of creating effective charts is choosing which type of data visualization works best for your needs.

Organize your process with design thinking: Design thinking breaks down the design process into five stages: empathize, define, ideate, prototype, and test.

Consider pre-attentive attributes: Pre-attentive attributes like marks and channels are the elements of a data visualization that people recognize automatically without conscious effort.

Avoid misleading or deceptive charts: It’s important that the visualizations you create are communicating your data accurately and truthfully.

Prioritize accessibility: Make your visualizations accessible and useful to everyone in your audience by using labeling, text alternatives, text-based formats, and distinguishing and simplifying elements.

Apply design principles: There are nine principles of design that you should consider when designing your visualizations: balance, emphasis, movement, pattern, repetition, proportion, rhythm, variety, and unity.


Outliers are important to identify and keep in mind, but in some situations it helps to temporarily remove them from view. This won’t change the data itself, but it will make your chart more proportional and help you notice trends.


This scenario describes making trade-offs. A trade-off involves balancing various factors, often by prioritizing one element while sacrificing another, in order to arrive at the best possible result.

A dimension is a qualitative data type that can be used to categorize data. Some examples include customer names, product names, and locations. 

Encoding is the process of translating dimensions and measures into visual representations of data.



Z-shaped data arrangements
Essential statistics are placed in the upper left corner. Then, information is arranged in a Z shape, with the least important data positioned in the bottom right corner. This establishes a logical hierarchy that users can easily understand.


Essential data is the most prominent
To draw the user’s attention, the most important visualization is presented as the largest object on the dashboard. This interactive chart represents the highest priority daily sales data for stakeholders.

Legends explain data
Legends explain how data is encoded in a dashboard so that users can better interpret its insights. Legends are arranged close to the charts they are describing.


A bar chart is best for illustrating data with a changing variable. A grouped bar chart will visualize values of two categorical variables, so it is easier to make comparisons. Therefore, you could use a grouped horizontal bar chart to compare the average price per night in each neighborhood to the number of currently available rentals in those neighborhoods. 


Legends explain how data is encoded in a dashboard so that users can better interpret its insights. The legend in your dashboard describes what each dot size on the map means. 


Filtering can help highlight certain elements of your dashboard, avoid clutter without deleting content, and increase your dashboard’s processing speed.



Processing speed describes how quickly a program can update and load a specified amount of data.



Reduce processing load and maintain dashboard effectiveness

Reduce processing load
One of the primary ways you can work to optimize your processing speed is by reducing the processing load. You can do this by:

- Pre-aggregating: 
- Using JOINs
- Filtering
- Linking to external locations;
- Avoiding user-defined functions
- Deciding between data views and tables




Case study: FeatureBase, Part 3: Exploring the trends with visualizations:


Company background
As a refresher, FeatureBase is an OLAP database company that enables businesses to gain insights from real-time analytics and AI. Their core technology, FeatureBase, is the first OLAP database built entirely on bitmaps that power real-time analytics and machine learning applications by simultaneously executing low latency, high throughput, and highly concurrent workloads. FeatureBase is sold to their clients, who become part of the sales cycle. This cycle includes the first point of contact with the potential customer to the moment they sign the purchasing contract and begin using FeatureBase



The challenge(s)
As you learned in 
the previous case study
, the FeatureBase sales team realized that they didn’t have the data they needed to find when customers were falling off. To fix this, they recreated their original sales funnel with new attributes that helped track customers at every stage of the sales cycle.

Their next challenge was deciding the best way to visualize the problem for the sales team.

The approach
FeatureBase selected an informal series of simple charts rather than a dashboard to help make fast decisions. This allowed the sales team to find the most difficult stage of the sales cycle in a matter of minutes, rather than requiring at least a week of dashboard creation. While many complex business questions are best answered with a carefully-crafted dashboard, this problem required a simple visual solution.

The results
Once the sales team knew that technical validation was the likely stopping point for their customers, they wondered if only certain types of potential customers were experiencing difficulty. They created a bar chart that represented different channels, or types of clients.

With this bar chart, FeatureBase’s team found that companies or points of contact who had non-technical backgrounds were the ones that were less likely to proceed to the contract stage. Then the sales team made a pie chart to confirm their suspicions. This pie chart offered evidence that an evaluator with a non-technical background was less likely to sign contracts.



Row-level security allows you to configure settings for rows within a dataset.

Object-level security can configure access to a single object. Additionally, it can enable extensive access or restrict access altogether.

Public availability is the least restrictive security setting, allowing access to anyone.


To create a dashboard that is accessible to anyone, the public availability privacy setting should be used.

They want to control the availability of a single item, such as a table, dataset, or single visualization. This is the purpose of object-level privacy permission. 

A business intelligence professional sets up row-level permissions in the database. This privacy setting controls the availability of specific rows of a table or dataset in a dashboard.


Encoding: The process of translating dimensions and measures into visual representations of the data

Measure: A quantitative data type that can be either discrete or continuous

Object-level permission: A privacy setting that controls the availability of a single item in a dashboard

Pre-aggregation: The process of performing calculations on data while it is still in the database

Processing speed: How quickly a program can update and load a specified amount of data 

Public availability: A privacy setting that allows anyone to access a dashboard

Row-level permission: A privacy setting that controls the availability of specific rows of a table or dataset in a dashboard

Trade-off: Balancing various factors, often by prioritizing one element while sacrificing another, in order to arrive at the best possible result




----------------------------------------------------------------------------
Module 3: Automate and monitor

Potential pain points when long-term monitoring:

Business intelligence monitoring involves building and using hardware and software tools to easily and rapidly analyze data and enable stakeholders to make impactful business decisions. As you design dashboards for long-term monitoring, there are a few common obstacles users might encounter that can make dashboards harder for them to use. In this reading, you’ll get a short checklist of potential obstacles, known as “pain points.” These are things to keep in mind when you build out the case study dashboard—and as you build dashboards in the future! This will help you build more user-friendly dashboards that your team and stakeholders can use long-term.

Three possible obstacles for long-term monitoring

1. Poorly defined use cases: The ways a business intelligence tool is actually used and implemented by the team are referred to as “use cases.” When designing a dashboard that includes live-monitoring, it’s important to establish how the different views will be used. For example, if you only include one “executive view” with no way to drill down into specific information different users might need, it leaves a lot of the interpreting work to users who may not understand or even need to understand all of the data.

2. Isolated snapshots: Snapshots of the latest information can be useful for reports, but if there’s no way to track the data’s evolution, then these snapshots have a pretty limited utility. Building in tracking for users to explore will help them understand the snapshots better. Basically, tracking means including insights about how the data is changing over time.

3. Lack of comparisons: When creating a dashboard, implementing comparisons can help users understand whether the visualizations being presented indicate good or bad performance. Comparisons place KPIs side-by-side in order to easily examine how similar or different they are. Similar to adding more context to snapshots, adding comparisons is a fast way to ensure users understand why the data in the dashboard is useful.



A circle chart could use color and size to compare the holidays with the highest traffic. The color would represent the holiday, while the size of each circle would represent the traffic volume on that holiday. The use of size and color in this way demonstrates relationships between numeric data and presents it in a compact format. 

Captions can make your charts more accessible to everyone. Tableau has a built-in caption generator that automatically describes the details of your chart. 

This chart answers the stakeholder’s question about the effect that weather has on traffic. The following charts would also help answer your stakeholder’s questions: traffic volumes by month per year, traffic volumes by hour, and holidays with highest traffic.



Place your most important chart into your dashboard first and place the other charts around it. You can also use design elements to create a logical flow in your dashboard, arrange the legends along with the charts, and evaluate and revise your mockup, if necessary.


Iterate on a dashboard:
You'll be tasked with incorporating feedback from the Minnesota Department of Transportation, requiring you to think critically about implementation.

This might involve adjusting existing elements or adding new ones based on the feedback provided.


There are several types of buttons you can add to your dashboard. You can add images with the images button, a navigational menu with the navigation button, and a download link with the download button. All of these components can help you create a more complete story with your visualizations.



----------------------------------------------------------------------------
Module 4: Present business intelligence insights

Business intelligence presentation examples:

Tips for building a presentation

Use the following tips and sample layout to build your own presentation.

1. Tip 1: Always remember audience and purpose
To develop an effective presentation that communicates your point, it’s important to keep your audience in mind. Ask yourself these two questions to help you define the overall flow and build out your presentation:

Who is my audience?

If your intended audience is primarily high-level executives, your presentation should be kept at a high level. Executives tend to focus on main takeaways that encourage improving, correcting, or inventing things. Keep your presentation brief and spend most of your time on results and recommendations, or provide a walkthrough of how they can best use the tools you’ve created. It can be useful to create an executive summary slide that synthesizes the whole presentation in one slide.

If your intended audience is comprised of stakeholders and managers, they might have more time to learn about new processes, how you developed the right tools, and ask more technical questions. Be prepared to provide more details with this audience!

If your intended audience is comprised of analysts and individual contributors, you will have the most freedom—and perhaps the most time—to go in to more detail about the data, processes, and results.

Support all members of your audience by making your content accessible for audience members with diverse abilities, experiences, and backgrounds.



What is the purpose of my presentation?:
If the goal of your presentation is to request or recommend something at the end, like a sales pitch, you can have each slide work toward the recommendations at the end.

If the goal of your presentation is to focus on the results of your analysis, each slide can help mark the path to the results. Be sure to include plenty of views of the data analysis steps to demonstrate the path you took with the data.

If the goal of your presentation is to provide a report on the data analysis, your slides should clearly summarize your data and key findings. In this case, it is alright to simply offer the data on its own.

If the goal of your presentation is to showcase how to use new business intelligence tools, your slides should clearly showcase what your audience needs to understand to start using the tool themselves.


2. Tip 2: Prepare talking points and limit text on slides
As you create each slide in your presentation, prepare talking points (also called speaker notes) on what you will say.

Don’t forget that you will be talking at the same time that your audience is reading your slides. If your slides start becoming more like documents, you should rethink what you will say so that you can remove some text from the slides. Make it easy for your audience to skim read the slides while still paying attention to what you are saying. In general, follow the five-second rule. Your audience should not be spending more than five seconds reading any block of text on a slide.


3. Tip 3: End with your recommendations
Ending your presentation with recommendations and key takeaways brings the presentation to a natural close, reminds your audience of the key points, and allows them to leave with a strong impression of your recommendations. Use one slide for your recommendations at the end, and make them clear and concise. And, if you are recommending that something be done, provide next steps and describe what you would consider a successful outcome.



4. Tip 4: Allow enough time for the presentation and questions
Assume that everyone in your audience is busy. Keep your presentation on topic and as short as possible by:

Being aware of your timing. This applies to the total number of slides and the time you spend on each slide. A good starting point is to spend 1-2 minutes on summary slides and 3-5 minutes on slides that generate discussion.  

Presenting your data efficiently. Make sure that every slide tells a unique and important part of your data story. If a slide isn’t that unique, you might think about combining the information on that slide with another slide.

Saving enough time for questions at the end or allowing enough time to answer questions throughout your presentation.


Putting it all together: Your slide deck layout:

1. First slide: Agenda 
Provide a high-level bulleted list of the topics you will cover and the amount of time you will spend on each. Every company’s practices are different, but in general, most presentations run from 30 minutes to an hour at most. Here is an example of a 30-minute agenda:

Introductions (4 minutes)

Project overview and goals (5 minutes)

Data and analysis (10 minutes)

Recommendations (3 minutes)

Actionable steps (3 minutes)

Questions (5 minutes)


2. Second slide: Purpose
Not everyone in your audience is familiar with your project or knows why it is important. They didn’t spend the last couple of weeks thinking through the BI processes and tools for your project like you did. This slide summarizes the purpose of the project for your audience and why it is important to the business.

Here is an example of a purpose statement:

Service center consolidation is an important cost savings initiative. The aim of this project is to monitor the impact of service center consolidation on customer response times for continued improvement. 


3. Third slide: Data/analysis
When discussing the data, the BI processes and tools, and how your audience can use them, be sure to include the following:

Slides typically have a logical order (beginning, middle, and end) to fully build the story. 

Each slide should logically introduce the slide that follows it. Visual cues from the slides or verbal cues from your talking points should let the audience know when you will go on to the next slide. 

Remember not to use too much text on the slides. When in doubt, refer back to the second tip on preparing talking points and limiting the text on slides. 

The high-level information that people read from the slides shouldn’t be the same as the information you provide in your talking points. There should be a nice balance between the two to tell a good story. You don’t want to simply read or say the words on the slides.

For extra visuals on the slides, use animations. For example, you can:

Fade in one bullet point at a time as you discuss each on a slide.

Only display the visual that is relevant to what you are talking about (fade out non-relevant visuals).

Use arrows or callouts to point to a specific area of a visual that you are using.


4. Fourth slide: Recommendations
If you have been telling your story well in the previous slides, the recommendations will be obvious to your audience. This is when you might get a lot of questions about how your data supports your recommendations. Be ready to communicate how your data backs up your conclusion or recommendations in different ways. Having multiple words to state the same thing also helps if someone is having difficulty with one particular explanation.


5. Fifth slide: Call to action
Sometimes the call to action can be combined with the recommendations slide. If there are multiple actions or activities recommended, a separate slide is best. 

Recall our example of a purpose statement: 

Service center consolidation is an important cost savings initiative. The aim of this project is to monitor the impact of service center consolidation on customer response times for continued improvement. 



Case study: Ipsos - Informing stakeholders with compelling data visualizations:

Company background
Ipsos is a globally-operating market research and polling company. The company builds solutions that provide insights into the actions, opinions, and motivations of citizens, consumers, patients, customers, and employees. Its solutions are based on data from surveys, social media monitoring, and qualitative or observational techniques.

The project details
The primary goal of the marketing team’s presentation was to communicate their insights to their stakeholders.

To agree on project details, the Ipsos account team met with the client team. They agreed on a list of priorities for charts and visualizations, the content of the dashboard, and the basic aesthetic and usability elements.

The Ipsos team used Looker as its dashboard platform, since it would be compatible with client’s existing Google Cloud infrastructure. This meant that Ipsos could avoid additional development on the data structure and create a dashboard that the client team would be most familiar with.

Ipsos built an advertisement dashboard that tracked the number of visitors and impressions that the client’s ads received. It also measured how long people watched the ads, how many purchases resulted from the ad, how much profit the ads incurred, and more.

The presentation approach
When preparing for the presentation, Ipsos decided to take a more conversational and interactive approach. They felt the dashboard should be experienced by its potential users as soon as possible. To do this, Ipsos made an effort to authorize and authenticate the stakeholder team’s users in advance. By the time Ipsos began their presentation, the client team could use and interact with the dashboard in real time.

In the initial meeting, the Ipsos team described the output of the dashboard and demonstrated how the user interface worked. Ipsos focused the meeting on reporting functionality and saved the more technical content for follow-up meetings. 

In these meetings, Ipsos invited other members of the client team and focused on other content. This content included the scripting, coding, and other technical aspects that wouldn’t be appropriate for the core team.

In each meeting, Ipsos used their two-part plan of providing a demonstration first and then allowing the users to experience the platform themselves. It was helpful for everyone to watch the Ipsos expert describe and interact with the user interface before the interactive session.

The feedback
At the end of the presentation, the client’s representatives shared positive comments about Ipsos’ work. They also mentioned a few aspects of the dashboard that they wanted to optimize.

After the presentation, the stakeholders organized their feedback in writing and delivered it to the Ipsos team. The feedback addressed two areas: the potential limitations of the platform for user data ingestion and the opportunities to further refine and develop the dashboard.

With this feedback, Ipsos could iterate on the dashboard tool and continue collaborating with their client.

Lessons learned
Key lessons learned and actions Ipsos would repeat include:

Establishing appropriate expectations and communicating frequently

Including their audience in the creation of presentation content

Encouraging participation during the presentation and identifying ways to involve the audience

Demonstrating the dashboard during the presentation and allowing the audience to make data requests to the presenter

Highlighting essential content and user benefits in the initial presentation, then focusing on highly technical content in subsequent meetings


Your presentation’s key insights’ slide should summarize some insights from your visualization. Your presentation should also include an intro slide, a summary of the business questions you answered, and your methods for creating your project. 



Activity Exemplar: Design a slide deck for a business intelligence presentation:

Introduction slide
Your first slide should include the following components:

A title

A visual element

Optional: a subtitle

Optional: Your name or company

In this exemplar, Slide 1 has the title “Minnesota Interstate Traffic Volume,” the subtitle “An in-depth analysis of traffic patterns on our interstate highways,” the Minnesota Department of Transportation logo, and a graphic of a car. It includes a simple color palette but few other details that distract from the introductory slide.

Business problem
In this exemplar, Slide 2 includes a very brief description of the business problem. The slide includes a picture of the Minnesota interstate with high traffic congestion. Some projects you work on might require more slides devoted to describing the business problem, but this simple slide is enough of a summary in this case.

Methods
Slide 3 of the exemplar describes the three primary charts used to visualize the data: Monthly Volumes, Weather, and Holiday Travel. It also includes a short description for each chart that suggests reasoning for separating the data into its own chart.

Insights
Slides 4–6 summarize the most important insights from the dashboard. Each slide features a chart that was introduced in Slide 3. These slides include images of each of the charts.

Slide 4 explains that August generally has the highest traffic volume, and February has the lowest. It points out an unusual dip in traffic in April 2018 and that aside from this dip the traffic trends are consistent from year to year.

Slide 5 focuses on weather and its effects on traffic. It also introduces a question: : What can the Minnesota Department of Transportation do to alleviate traffic issues on rainy days?

Slide 6 explains the holiday traffic trends. It supposes that the high traffic in August might be partially due to the state fair that takes place during that month. It also suggests that the holidays in January contribute to most of the traffic volume during that month.

Slide 7 examines when hourly traffic patterns emerge and if holidays impact them. It also explains that while to- and from-work commuter traffic has higher volume, there is not a significant difference during a holiday.

Slide 8 is the final slide and includes a quick signoff to conclude the slide deck presentation. This slide also helps the presenter transition into asking the audience if they have any questions.



Use prediction for better collaboration:
Predictive analytics
Predictive analytics is a branch of data analytics that uses historical data to identify patterns to forecast future outcomes that can guide decision-making. The goal of predictive analytics is to anticipate upcoming events and preemptively make decisions according to those predictions. The predictions can focus on any point in the future—from weekly measurements to revenue predictions for the next year. 

By feeding historical data into a predictive model, stakeholders can make decisions that aren’t just based on what has already happened in the past—they can make decisions that take into account likely future events, too! 

Presenting dashboards
As a BI professional, you might not be performing predictive analytics as part of your role. However, the tools you build to monitor or update data might be helpful for data scientists on your team who will perform this kind of analysis. By presenting dashboards effectively, you can properly communicate to stakeholders or data scientists what the next step will be in the data pipeline, and set them up to take the tools you create to the next level. 


Fine-tuning your resume
One of the most important ways you can adapt your existing resume is by making it specifically tailored to BI roles. Below are links to two resume examples. The first example is a rough draft an entry-level BI professional created early on in her resume writing process. The second resume is her final draft. This version is more specific about the roles she is interested in and how her previous experience can be applied to BI roles. Refer to both versions below:




----------------------------------------------------------------------------
Module 5: Course 3 end-of-course project

Course 3 End-of-course project overview: Google Fiber

Now, use the target table you created to design a BI visualization that will address the Google Fiber customer service team’s questions. As you create your dashboard, complete the steps of the process you’ve learned in this course. Start by creating a mockup, then make your charts, a dashboard, and a brief presentation that summarizes your work. At any time, feel free to review the activities from earlier in this course.

Step 1: Load your data into Tableau
In the previous course, you created a reporting table that merged all three of the tables you were provided. Now, open your reporting table in Tableau to begin building your visualizations.

Step 2: Create a mockup

Step 3: Create your charts

Step 4: Organize your dashboard

Step 5: Complete your executive summary


Activity Exemplar: Build a dashboard for Google Fiber:
https://public.tableau.com/app/profile/googlebicert/viz/GoogleBusinessIntelligenceCertificateGoogleFiberExemplar/RepeatCalls


Repeats by Month
The first tab of the dashboard includes two bar charts: the first chart visualizes the number of repeat calls the customer service team received each month. Contacts_N represents the first date a customer called– then, you can explore how often the customer called again that week. For example, 1,636 customers called again one day after their initial call, but only 575 customers called again seven days later in January. 

The second chart visualized the percentage of first contact calls by day of the week; in January, only 8.71% of customers made first contact on Sunday. The majority of customers reached out for the first time on Monday in January! 

Tables
The second tab of the dashboard includes two tables: Repeat Calls by First Call Date and Calls by Market and Type. 

The first table allows stakeholders to explore the number of different types of calls by date. The second table then separates calls into market and problem type to provide more specific information about what markets experience the most calls and the problems customers have that seem to prompt repeat calls. 

Market and Type for First Repeat Calls
The Market and Type for First Repeat Calls uses the data from the previous tabs table in order to further visualize the problem types that seem to generate the most repeat calls for different markets. 



Calls Across Q1
The final dashboard tab includes two charts to visualize the number of Day 0 calls across markets and problem types and the first repeat calls across markets and problem types. This helps users gain insight into what markets and problems are generating calls in the first quarter of the year, as well as which ones are prompting customers to call again after the first contact. 


This exemplar is only one way to complete the Google Fiber project. When comparing your work to this exemplar, use it as an example to guide your process instead of an ideal to replicate. Make sure to also explore the dashboard on Tableau Public to get a better understanding of its interactive components. Then, finalize your executive summary document so you can share your work on your professional portfolio.


----------------------------------------------------------------------------
Module 6: Put your Business Intelligence Certificate to work

